---
title: "ARE 213 PS 2b"
author: "S. Sung, H. Husain, T. Woolley, A. Watt"
email: aaron@acwatt.net
date: "2021-11-23"
output:
  pdf_document:
    toc: true
    toc_depth: 2
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{amsmath}
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
   - \usepackage{float}
   - \floatstyle{plaintop}
   - \restylefloat{table}
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'ARE_213_PS2b.pdf')) })
---

```{r setup, message=F}
rm(list=ls())
knitr::opts_chunk$set(echo = F)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps2b for ARE 213

Notes: Need to open ps2b.Rmd from the ps2b folder (without Rstudio being started first)
if Rstudio is already started, the working directory will not be set
to the ps2b/ folder

\usepackage{dcolumn}: dcolumn is needed in latex compilation for multicolumn tables
-->

```{r Settings}
# stargazer table type (html, latex, or text)
# Change to latex when outputting to PDF, html when outputting to html
table_type = "latex"
Cache = TRUE
```




```{r packages}
# install.packages("Synth")
library(haven)
library(stargazer)
library(ggplot2)
library(tinytex)
library(kableExtra)
library(fastDummies)
library(Synth)
library(progress)
library(tidyverse)
```



\newpage
<!--=========================================================================-->
# Problem 1
<!--=========================================================================-->
**We first estimate an event study specification.**

## Part (a)
**First determine the minimum and maximum event time values that you can
estimate in this data set. Code up a separate event time indicator for each
possible value of event time in the data set. Estimate an event study regression
using all the event time indicators. What happens?**
\vspace{1em}


```{r Load Data, eval=T, cache=Cache}
# Load data from PS2a with previous log variables
data = read_dta('traffic_safety2.dta') %>%
    mutate(fat_pc = fatalities/population,
           ln_fat_pc = log(fat_pc),
           ln_tvmt_pc = log(totalvmt/population),
           ln_precip = log(precip),
           ln_rspeed = log(rural_speed), 
           ln_uspeed = log(urban_speed)) %>%
    data.frame(.)
```


Table \ref{tab:max-min-event-times-table} lists the minimum and maximum event
times that exist in the data for states that enacted a primary seat belt law in
our study period (1981-2003).

```{r Determine event time values}
# Create list of event dates for states that passed primary laws in our study
event_dates = data %>%
    group_by(state) %>%
    mutate(event = primary - lag(primary), # event=1 ==> first year primary=1
           event_year = year) %>%
    filter(event == 1) %>%
    select(state, event_year)

# Add year of primary event and event time (t) to dataframe
data = data %>%
    left_join(event_dates, by='state') %>%
    mutate(j = ifelse(is.na(event_year), 99, year - event_year))
    # t = 99 ==> control state (doesn't pass primary during study period)

# Table of max and min event times
# Shouldn't these be our event study thresholds?
df_temp = data %>%
    filter(j < 99) %>%
    group_by(state) %>%
    summarize(min_j = min(j), max_j = max(j))

max_j_inclusive = min(df_temp$max_j, na.rm = T)
min_j_inclusive = max(df_temp$min_j, na.rm = T)
max_j = max(filter(data, j<99)$j, na.rm = T)
min_j = min(filter(data, j<99)$j, na.rm = T)
```



```{r max-min-event-times-table}
data.frame(max_j = max_j, min_j = min_j) %>%
  kbl(caption = "Maximum \\& Minimum Event Time Values",
      col.names = c('Max j', 'Min j'),
      align = 'cc') %>%
  kable_styling(latex_options = "HOLD_position")
```

Notice from Figure \ref{fig:max_event_time_distribution} that we have a wide
range of maximum event times across our panel of states. One state even has a
maximum event time of 0 -- meaning they only added primary seat belt laws in the
last year of our panel (2003). This means we will have an unbalanced panel if we
run a regression on all possible event time dummies. In fact, we will still have
an unbalance panel if we create max and min event time bins to aggregate early
and late periods (an indicator for a event times greater than 5 and an indicator
for all event times less than -5), we will still have an unbalanced panel.

```{r max_event_time_distribution, message=F, fig.width=4, fig.height=3, fig.align='center', fig.cap='Treated States, maximum event time histogram'}
df_temp %>%
    arrange(max_j) %>%
    filter(!is.na(max_j), max_j < 99) %>%
    select(max_j) %>%
    ggplot(aes(x=max_j), data=.) +
    geom_histogram() +
    xlab("max event period")
```

To estimate the event study treatment effects, corresponding regression equation is:
\[
Y_{st} = \alpha + \sum\limits_{j=min\_t}^{max\_t} \tau_j D_{jst}
    + \gamma_s + \delta_t + \varepsilon_{st} + u_{st}
\]
Note that we are estimating the regression with state and year fixed effects. In
practice, we would want to omit a specific event time indicator so all our treatment
effects are measured with respect to that event time. If we keep all our indicators,
then R will implicitly choose which to event time indicator to omit for us because
the event time dummies, along with state and year fixed effects, are colinear.


```{r create event time dummies}
# Function for adding dummies to a dataframe for all uniuqe values between given numbers
create_dummies = function(df, colname, min_value, max_value) {
    # Create dummies for each value of colname between min_value and max_value
    df1 = df
    for (val in min_value:max_value) {
        df1 = mutate(df1, "{colname}_{val}" := ifelse(eval(as.symbol(colname)) == val, 1, 0))
    }
    return(df1)
}

# Create order of dummies for dataframe (then used for regression table)
name_order1 = paste('j', min_j_inclusive:max_j_inclusive, sep='_')
# Create Dummies that make a balanced panel
# (only dummies for event times j that are shared across all states)
df_few_dummies = create_dummies(data,
                             colname = 'j',
                             min_value = min_j_inclusive,
                             max_value = max_j_inclusive) %>%
    relocate(all_of(name_order1)) %>% 
    # Change "j_..." to "..._ET" because LaTeX doesn't like j_-3 type variable names
    rename_with(~ paste0(str_replace(., 'j_', ''), '_ET'), contains("j_"))

# Create order of dummies for dataframe (then used for regression table)
name_order2 = paste('j', min_j:max_j, sep='_')
# Create Dummies for all possible event times
# (results in unbalanced panel over event times j)
df_all_dummies = dummy_cols(data, select_columns = 'j') %>%
    select(-j_99) %>%
    filter(state != 99)  %>%
    relocate(all_of(name_order2)) %>% 
    rename_with(~ paste0(str_replace(., 'j_', ''), '_ET'), contains("j_"))
```

We can see column (1) of  Table \ref{tab:event-study-dummy-trap} that the indicator for event time +19
was omitted for the regression. However, for interpretability, we'd rather have
the treatment effects relative to a period closer to the year of initial treatment.

```{r Run dummy-trap event study regression}
reg_1a = df_all_dummies %>%
    mutate(state=factor(state), year=factor(year)) %>%
    select(ln_fat_pc, state, year, contains('_ET')) %>%
    lm(ln_fat_pc ~ ., data = .)

```



<!--
Notes:
the panel is very unbalanced. Even after using -5 and +5 thresholds, there are
still 5 states that have max event time less than 5. What happens to these?

order of the regression table is not correct, despite using the `order` arg.

Order the columns of the dataframe, then hopefully will be in order for regression
and for plotting
-->
















## Part (b)
**Estimate another event study regression using all the event time indicators
save one that you choose to omit. Generate a plot of the event study
coefficients.**

```{r Omit t-1 dummy}
reg_1b = df_all_dummies %>%
    mutate(state=factor(state), year=factor(year)) %>%
    select(ln_fat_pc, state, year, contains('_ET'), -`-1_ET`) %>%
    lm(ln_fat_pc ~ ., data = .)

```

We have chosen to omit the event time -1 from the regression so the other event
time inidicator coefficients can be interpreted as relative to the year immediately
before the passage of the primary seat belt law.
Column (2) of Table \ref{tab:event-study-dummy-trap} shows that the -1 event time
period was omitted, and we we see that all of the treatment effects occuring before
event time -1 are not significantly different from zero, whereas all the event time
coefficient estimates for after event time -1 are negative and all are signficantly
less than zero starting with event period 5.

```{r event-study-table, results='asis'}
stargazer(reg_1a, reg_1b,
          title = "Event Study Regressions\\label{tab:event-study-dummy-trap}",
          dep.var.caption = "Log(Fatality per Population)",
          dep.var.labels.include = FALSE,
          column.labels = c("Event Study a", "Event Study b"),
          omit = c("state", "year"),
          add.lines=list(c('Chose dummy to omit', 'No', 'Yes')),
          font.size = "footnotesize", column.sep.width = "1pt", no.space = TRUE,
          omit.stat=c("f", "ser"),
          single.row = TRUE,
          digits = 4, type = table_type, header = FALSE)


```










\newpage
## Part (c)
**Create minimum and maximum event time indicators that correspond to bins of
event time < -5 and event time > 5 respectively. Appropriately specify and
estimate an event study regression using these min and max event time
indicators. Generate a plot of the event study coefficients. Explain which
specification you prefer, this one or the one in part (b).**


```{r Create threshold event time dummies}
# Function for adding dummies to a dataframe for all uniuqe values between given numbers
create_dummies_threshold = function(df, colname, min_value, max_value, suffix = NULL) {
    # Create indicator variables for each value of colname between min_value and max_value
    # then create indicator variables for all values of colname below min_value
    # and another for above max_value
    if (is.null(suffix)) {suffix = colname}
    df1 = df
    # add aggregate indicator for all values below min_value
    df1 = mutate(df1, "below_{suffix}" := ifelse(eval(as.symbol(colname)) < min_value, 1, 0))
    # add all indicators in between min and max_value
    for (val in min_value:max_value) {
        df1 = mutate(df1, "{val}_{suffix}" := ifelse(eval(as.symbol(colname)) == val, 1, 0))
    }
    # add aggregate indicator for all values above max_value
    df1 = mutate(df1, "above_{suffix}" := ifelse(eval(as.symbol(colname)) > max_value, 1, 0))
    return(df1)
}


# Create Dummies that make a balanced panel
# (only dummies for event times j that are shared across all states)
df_threshold_dummies = create_dummies_threshold(data,
                                          colname = 'j',
                                          min_value = -5,
                                          max_value = 5,
                                          suffix = 'ET')

```

```{r Regression with threshold event time dummies}
reg_1c = df_threshold_dummies %>%
    mutate(state=factor(state), year=factor(year)) %>%
    select(ln_fat_pc, state, year, contains('_ET'), -`-1_ET`) %>%
    lm(ln_fat_pc ~ ., data = .)

```


```{r Event Study table with Thresholds, results='asis'}
stargazer(reg_1c,
          title = "Event Study Regression with Threshold Indicators\\label{tab:event-study-thresholds}",
          dep.var.caption = "Log(Fatality per Population)",
          dep.var.labels.include = FALSE,
          column.labels = c("Event Study c"),
          omit = c("state", "year"),
          add.lines=list(c('Chose dummy to omit', 'Yes'), c('Agg. Threshold Indicators', 'Yes')),
          font.size = "footnotesize", column.sep.width = "1pt", no.space = TRUE,
          omit.stat=c("f", "ser"),
          single.row = TRUE,
          notes = c("below\\_ and above\\_ variables are aggregate indicators for",
                    "all event times that are below -5 and above 5, respectively."),
          digits = 4, type = table_type, header = FALSE)

plot_text1 = "(red) Plot of all possible event-time coefficients and 95\\% confidence intervals from Table \\ref{tab:event-study-dummy-trap}; (blue) plot of event-time coefficients and 95\\% confidence intervals for event-times from years before the passage of the primary seat belt law to 5 years after and coefficents for aggregate event-time indicators of more than 5 years before and more than 5 years after the law passed, plotted at x-values of -6 and 6, respectively (from Table \\ref{tab:event-study-thresholds})"

```



```{r Event Study Plot with Thresholds, fig.width=8, fig.cap=plot_text1}
new_varname = function(oldname, min_value, max_value) {
    name = str_replace_all(oldname, "`","")
    name = str_replace(name, "_ET", "")
    name = str_replace(name, "below", paste("<", min_value))
    name = str_replace(name, "above", paste(">", max_value))
    return(name)
}

xvalue = function(oldname, min_value = NULL, max_value = NULL) {
    name = str_replace_all(oldname, "`","")
    name = str_replace(name, "_ET", "")
    if (!is.null(min_value)) {
        name = str_replace(name, "below", as.character(min_value - 1))
        name = str_replace(name, "above", as.character(max_value + 1))
    }
    return(as.numeric(name))
}

regression_dataframe = function(reg, reg_type, min_value = NULL, max_value = NULL, suffix = 'ET') {
    df = data.frame(coef = names(reg$coefficients), 
                    value = reg$coefficients,
                    lower = confint(reg)[,1],
                    upper = confint(reg)[,2],
                    reg_type = reg_type) %>%
        filter(grepl(suffix, coef)) %>%
        mutate(x_tick = new_varname(coef, min_value, max_value),
               event_time = xvalue(coef, min_value, max_value)) %>%
        return()
}

df1 = rbind(
    regression_dataframe(reg_1b, "all indicators"),
    regression_dataframe(reg_1c, "max-min indicators", min_value = -5, max_value = 5)
)

df1 %>%
    ggplot(aes(x=event_time, y=value, color = reg_type)) + 
    geom_errorbar(aes(ymin=lower, ymax=upper), width=.1) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = 0) +
    labs(color = "Parameters estimated") +
    xlab("Event Time (years since passage of primary seat belt law)") +
    ylab("Coefficient Value") +
    annotate(geom = "segment", x = 3, y = -0.3, xend = 5.7, yend = -0.14,
             arrow = arrow(length = unit(2, "mm"))) +
    annotate(geom = "text", x = 3, y = -0.32, 
             label = "(blue) coeffcient on >5 years after passage", 
             hjust = "center") +
    annotate(geom = "segment", x = -4.5, y = -0.15, xend = -5.9, yend = 0.018,
             arrow = arrow(length = unit(2, "mm"))) +
    annotate(geom = "text", x = -8, y = -0.17, 
             label = "(blue) coeffcient on >5 years before passage", 
             hjust = "center")
    
```



<!--
We could add another regression here where we prune the data to be balanced in
event time with threshold dummies.

Also could do post-treatment event time indicators interacted with treatment cohort
indicators and plot a graph of the different cohort-specific treatment estimates
-->




\newpage
## Part (d)
**What happens to your estimates from part (b) if you exclude the “pure control”
states from your sample? What about if you exclude the pure controls in part
(c)?**


```{r Regressions excluding pure control states}
# Regression with all possible event-time indicators, removing pure control states
reg_1b2 = df_all_dummies %>%
    group_by(state) %>%
    filter(mean(primary) > 0) %>%
    mutate(state=factor(state), year=factor(year)) %>%
    select(ln_fat_pc, state, year, contains('_ET'), -`-1_ET`) %>%
    lm(ln_fat_pc ~ ., data = .)

# Regression with max, min aggregated event-time indicators, removing pure control states
reg_1c2 = df_threshold_dummies %>%
    group_by(state) %>%
    filter(mean(primary) > 0) %>%
    mutate(state=factor(state), year=factor(year)) %>%
    select(ln_fat_pc, state, year, contains('_ET'), -`-1_ET`) %>%
    lm(ln_fat_pc ~ ., data = .)

```


```{r Event study table dropping pure controls, results='asis'}
stargazer(reg_1b, reg_1b2, reg_1c, reg_1c2,
          title = "Event Study Regressions with and without Pure Control States\\label{tab:event-study-no-controls}",
          dep.var.caption = "Log(Fatality per Population)",
          dep.var.labels.include = FALSE,
          column.labels = c("All indicators",
                            "All indicators",
                            "Min-max indicators",
                            "Min-max indicators"),
          omit = c("state", "year"),
          add.lines=list(c('Agg. Threshold Indicators', 'No', 'No', 'Yes', 'Yes'), 
                         c('Include Pure Controls', 'Yes', 'No', 'Yes', 'No')),
          font.size = "footnotesize", column.sep.width = "1pt", no.space = TRUE,
          omit.stat=c("f", "ser"),
          single.row = TRUE,
          digits = 4, type = table_type, header = FALSE)


```



For the all-event-times-indicators regressions from part (b), we can see in
Table \ref{tab:event-study-no-controls} that for event-time indicators
after the primary seat belt law is passed (event times $\geq 0$), the
coefficients change from being all negative and mostly significant to being all
insignificant at the 0.1 level and mixed signs. Interestingly, the coefficients
on the event-time indicators for times before the passage of the law become
significantly negative -- indicating that the passage of the law might have
caused an increase in traffic fatalities in states in the treatment group (i.e.,
the effect of treatment on the treated might be the opposite sign we expect).
However, we also should note that the coefficient on event time 19 (19 years
after the passage of the law) is dropped by the regression because of
colinearity -- without pure control states, the combination of state and year
fixed effects with indicators for all but one event times becomes colinear.

Because we have omitted the coefficient on event time 19 and event time 0, we
cannot readily interpret the coefficients in column (2) of Table
\ref{tab:event-study-no-controls}.

If we focus on columns (3) and (4) from Table
\ref{tab:event-study-no-controls}, we can see removing the pure control
states (column 4) reduces the significance and magnitude of all our
coefficients, but the last two coefficients for event time 5 (`5\_ET') and the
aggregate of all event times greater than 5 (above\_ET) are still significant
and negative. One way to interpret this is that the treatment effect on the
treated is smaller than that of the average treatment effect. We also could
hypothesize that the states that passed the primary seat belt laws during our
study period were doing something other than passing the primary law and
something different from the pure control states to reduce traffic fatalities,
so our comparison within the treatment states (column 4) estimates a smaller
effect than when we compare our treated states to the pure control states.








\newpage
## Part (e)
**Overall, does the event study regression make you more confident or less
confident that seat belt laws reduce fatalities (relative to the fixed effects
results that you estimated on the last problem set)? Briefly explain.**

















\newpage
## Part (f$^*$)
**Building off the event study regression from part (c), estimate the
interaction weighted event study estimator from Sun and Abraham (2020). As a reminder, the
interacted event study regression takes the standard event time indicators
(without any binning) and interacts each one with a cohort indicator (a cohort
refers to a group of states that share the same date on which they were
first treated). You then form the estimate for event time coefficient $\tau_j$ 
by averaging the estimates of the cohort-specific $\tau_j$ using
the weights described in Sun and Abraham (2020).**


<!-- A FEW TIPS:
• Given the interacted specification’s complexity and the potential
colinearities involved, you may find it easier to manually generate the needed
interaction terms rather than using factor-variable commands in Stata/R (e.g.
Stata’s xi com- mand).

• Some of the interactions between cohort indicators and event time indicators
will generate constants, because the event time indicator is too early or too
late to be identified for a given cohort. Think carefully about this.'

• In the case of the coefficients for j < -5 and j > 5, note that you will be
averaging across cohorts and across event time coefficients.

• You will need to compute the share that each cohort contributes to each
event time indicator to get the right weights. E.g. suppose that the event time
indicator for j = +5 were informed only by cohort 1, containing 3 states, and
cohort 2, containing 1 state. Then the weights for event time indicator j = +5
would be 0.75 for cohort 1 and 0.25 for cohort 2.

• The paper references an eventstudyweights Stata package to estimate the
cohort-by-event-time weights. Be aware that (as of 2020) this package computes
the implicit weights that a standard event study regression applies to each of
the cohort-by-event time coefficients, not the weights that should be used to
generate the interaction weighted event study estimator.
-->

```{r fig text2}
plot_text2 = "Plot of Sun-and-Abraham(2020)-style cohort-by-event-time coefficients."
```


```{r interacted weighted event study, fig.width=8, fig.cap=plot_text2}
# Function for adding dummies to a dataframe for all interactions between two columns
create_interacted_indicators = function(df, col1, col2) {
    # Create dummies for each combination of non-constant interactions of col1 and col2
    df1 = df %>% mutate("{col1}" := ifelse(eval(as.symbol(col1)) == 99, NA, eval(as.symbol(col1))),
                        "{col2}" := ifelse(eval(as.symbol(col2)) == 99, NA, eval(as.symbol(col2))))
    df1 = df
    df1[, col1] = replace(df1[, col1], df1[, col1]==99, NA)
    df1[, col2] = replace(df1[, col2], df1[, col2]==99, NA)
    for (val1 in df1[, col1] %>% unique() %>% sort()) {
        for (val2 in df1[, col2] %>% unique() %>% sort()) {
            inter = as.integer(df1[, col1] == val1) * as.integer(df1[, col2] == val2)
            inter = replace_na(inter, 0)
            # Only keep the interaction if there is variation in the data
            if (var(inter) > 0) {
                name1 = ifelse(val1<0, paste0('n', abs(val1)), val1)
                name2 = ifelse(val2<0, paste0('n', abs(val2)), val2)
                name = paste('inter', name1, name2, sep='_')
                df1[, name] = inter
            }
        }
    }
    return(df1)
}

reg_1f = data %>%
    filter(state != 99) %>%
    create_interacted_indicators(., 'j', 'event_year') %>%
    mutate(state=factor(state), year=factor(year)) %>%
    select(ln_fat_pc, state, year, contains('inter_'), -contains('_n1_')) %>%
    lm(ln_fat_pc ~ ., data = .)

reg_1f_df = data.frame(summary(reg_1f)$coefficients) %>%
    mutate(var = row.names(.)) %>%
    # select(var, everything()) %>%
    rename(est = Estimate, SE = Std..Error, t = t.value, p = Pr...t..) %>%
    filter(grepl('inter_', var)) %>%
    separate(var, c('x', 'event_time', 'event_year')) %>%
    select(-x) %>%
    mutate(event_year = as.integer(event_year)) %>%
    mutate(event_time = as.integer(str_replace(event_time, 'n', '-')))

# Plot all event_year cohort treatment effects
reg_1f_df %>%
    ggplot(aes(x=event_time, y=est, color=factor(event_year))) +
    geom_point() + geom_line() +
    geom_hline(yintercept = 0) +
    labs(color='Treatment Cohort') +
    xlab("Event Time (years since passage of primary seat belt law)") +
    ylab("Coefficient Value")

plot_text3 = "Plot of cohort-weighted avereage event-time coefficents -- weighted using the number of states in each cohort"
```

```{r Plot of cohort-weighted event time treatment effects, fig.width=8, fig.cap=plot_text3}

# Calculate treatment effect weights for each combination of event time and event year
for (i in (1:nrow(reg_1f_df))) {
    t_ = reg_1f_df[i, 'event_time']
    y_ = reg_1f_df[i, 'event_year']
    # Find number of states in cohort event_year 
    numerator = data %>%
        filter(j == t_, event_year == y_) %>%
        nrow()
    denominator = data %>%
        filter(j == t_) %>%
        nrow()
    reg_1f_df[i, 'event_time_weight'] = numerator / denominator
}

# Calculate weighted average of treatment effects for a given event time '#7CAE00'
reg_1f_df %>%
    group_by(event_time) %>%
    summarize(value = weighted.mean(est, event_time_weight)) %>%
    mutate(reg_type = 'Sun & Abraham \n cohort weighted') %>%
    rbind(., select(df1, event_time, value, reg_type)) %>%
    arrange(reg_type) %>%
    # Plot the treatment effects over event time
    ggplot(aes(x=event_time, y=value, color = reg_type)) + 
    geom_line() +
    geom_point() +
    geom_hline(yintercept = 0) +
    labs(color = "Parameters estimated") +
    xlab("Event Time (years since passage of primary seat belt law)") +
    ylab("Coefficient Value")
```













\newpage
<!--=========================================================================-->
# Problem 2
<!--=========================================================================-->
**We now apply the synthetic control methods from Abadie et al (2010).**

<!--
Some preliminaries: Abadie et al have created a downloadable “canned”
command to run the synthetic control method. To download the com-
mand for Stata you will need to have an updated version of Stata and
be running Stata on a Mac, PC or Unix/Linux. R code is also available.
In Stata type ‘update all’ and then ‘update swap’. Next, go to the web-
site below and follow the instructions. There is also downloadable code
for Matlab at http://web.stanford.edu/~jhain/synthpage.html (note:
blindly copying and pasting this URL may not reproduce the tilde)
 -->


## Part (a)
**We created an aggregate “treatment” state (state number 99 or “TU”) which
combines the (population weighted) data from the first 4 states to have a
primary seatbelt law (CT, IA, NM, TX). Please use this state as the “treatment”
state in the synthetic control analysis.**

### ------ a.i
**Compare the average pre-period log traffic fatalities per capita of the TU
site to that of the average of all the “control” states. Next, graph the
pre-period log traffic fatalities by year for the pre-period for both the TU and
the average of the control group. Interpret.**

```{r Compare averages between TU and all controls}
# Calculate average pre-period log traffic fatalities per capita
preperiod_years = data %>%
    filter(state == 99, primary == 0) %>%
    select(year) %>%
    unlist(.) %>%
    as.numeric(.)

data2 = data %>%
    group_by(state) %>%
    mutate(control = ifelse(mean(primary) > 0, 0, 1),
           treated = ifelse(state == 99, 'Yes', 'No')) %>%
    filter(control == 1 | state == 99)

means = data2 %>%
    filter(year %in% preperiod_years) %>%
    group_by(control) %>%
    summarize(avg = mean(ln_fat_pc))

means %>%
    select(avg) %>%  # control = 0 on top ==> TU on top
    t() %>%  # control = 0 on left ==> TU on left
    kbl(caption = "Average Pre-period log(traffic fatalities per capita)\\label{tab:avg-fat-TU}",
        col.names = c('Aggregate Treatment State', 'Aggregate Control State'),
        row.names = F,
        align = 'cc') %>%
    kable_styling(latex_options = "HOLD_position")

```

<!-- Reference: Table \ref{tab:avg-fat-TU} -->

```{r Plot pre-trends, message=F}
# Plot log fat per cap for pre-treatment years, for treatment and control
data2 %>%
    filter(year %in% preperiod_years) %>%
    group_by(year, treated) %>%
    summarize(year_avg = mean(ln_fat_pc)) %>%
    ggplot(aes(x=year, y=year_avg, color=factor(treated))) +
    geom_line() +
    geom_point() +
    geom_hline(yintercept = filter(means, control==0)$avg, color='cyan') +
    geom_hline(yintercept = filter(means, control==1)$avg, color='red') +
    labs(color = "Will Enact\n Seatbelt Law") +
    xlab("Year") +
    ylab("log(Fatalities per capita)") +
    ggtitle("Pre-period annual avereage log(Fatalilties per capita)") +
    annotate(geom = "text", x = 1984, y = -1.56, 
             label = "Average for control\n states over pre-periods", 
             hjust = "center") +
    annotate(geom = "text", x = 1984, y = -1.34, 
             label = "Average for the aggregate\n treated unit over pre-periods", 
             hjust = "center")

```






\newpage
### ------ a.ii
**Compare the dependent variable between the TU site and each control state for
the year before the treatment. Which control state best matches the TU? Now
compare this state’s covariates with the TU covariates. Do they appear similar?
What might this imply for in terms of using this state as the counterfactual
state?**

```{r Compare dependent var, message=F}
# Compare log(fat per cap) for last pre-treatment year, for treatment and each control
treat_val = (data2 %>% 
    filter(year == tail(preperiod_years, n=1),
           state == 99))$ln_fat_pc

data2 %>%
    filter(year == tail(preperiod_years, n=1)) %>%
    filter(control == 1 | state == 99) %>%
    mutate(diff = abs(treat_val - ln_fat_pc)) %>%
    arrange(diff) %>%
    select(ln_fat_pc, diff) %>%
    # head() %>%
    kbl(caption = paste("Dependent variable in",
                        tail(preperiod_years, n=1),
                        "for treated (99) and control states\\label{tab:1985-depvar}"),
        col.names = c('State ID', 'log(fatatilities per captia)', 
                      'Absolute difference between this state and treated'),
        row.names = F,
        align = 'cc') %>%
    kable_styling(latex_options = "HOLD_position")
```
We can see from Table \ref{tab:1985-depvar} that state \# 47 is the control state
that is closest to the aggregate treatment state in the dependent variable.



```{r Compare other vars for 47 and 99, message=F}
# Compare covariates for last pre-treatment year, for treatment and control 47
data2 %>%
    filter(year == tail(preperiod_years, n=1)) %>%
    filter(state %in% c(47, 99)) %>%
    select(college, beer, population, unemploy, totalvmt, precip,
           snow32, rural_speed, urban_speed, fat_pc) %>%
    round(4) %>%
    t() %>%
    kbl(caption = paste("Dependent variable in",
                        tail(preperiod_years, n=1),
                        "for treated (99) and control states\\label{tab:1985-covar}"),
        col.names = c('Control State 47', 'Aggregate Treatment State'),
        align = 'cc') %>%
    kable_styling(latex_options = "HOLD_position")
```

From Table \ref{tab:1985-covar}, we can see that there are identical speed
limits between state 47 and the aggregate treatment state. But most of the other
covariates are very far apart in the distributions -- total vehicle miles
traveled is different by an order of magnitude and are in different sides of the
distribution; both snow and precipitation are on different sides of their
distributions; the unemployment rates, population levels, and college rates are
very far apart as well. This makes it hard to believe that state \#47 would be a
good control for our aggregate treatment state.

We also generated percentiles for each of the covariates by state and year.
Using the average pre-treatment percentiles of covariates, the two states do not
look similar at all. For instance, 

- college: WV is 1p and TU is 49p 
- beer: WV is 15p and TU is 92p 
- unemployment: WV is 99p and TU is 68p 
- ln_tvmt: WV is 20p and TU is 87p 
- ln_precip: WV is 64p and TU is 33p 
- snow: WV is 61p and TU is 33p. 

They only match percentiles for log rural speed and log urban speed (19 and 39).





\newpage
## Part (b)
**Apply the synthetic control method using the available covariates and
pre-treatment outcomes to construct a synthetic control group.**

### ------ b.i
**Discuss the synthetic control method including its benefits and potential
drawbacks.**


The synthetic controls approach circumvents the issue of not having plausible
control group by constructing one by weighting untreated units selected by
weighting covariates based on their predictability of the pre-treament outcome.
Even though this method does not calculate a standard error for the treatment
effect in the conventional sense, it does allow us to test the believability of
the treatment result via a placebo test (as long as the number of potential
control units is sufficiently large). Synthetic controls become virtually
impossible to use without sufficiently many potential controls because the
placebo test is vital to the interpretability of the treatment effect.
Essentially, synthetic controls is only as good as the number of potential
control units.








### ------ b.ii
**Use the software package provided by Abadie et al to apply the synthetic
control method. (You are free to use either Stata, Matlab, or R but answers will
be provided in Stata and R only). Please be sure to state precisely what the
command is doing and how you determined your preferred specification.**


```{r Aarons Synth Function wrapper, message=F}
apply_synth = function(data, predictors_, treated_unit=99) {
    # Create list of all years in sample
    all_years = data %>%
        arrange(year) %>%
        select(year) %>%
        unique() %>%
        unlist(.) %>%
        as.numeric(.)
    
    # Create list of pre-treatment years
    preperiod_years = data %>%
        filter(state == 99, primary == 0) %>%
        select(year) %>%
        unlist(.) %>%
        as.numeric(.)
    
    # Create list of control states
    control_states = data %>%
        group_by(state) %>%
        mutate(control = ifelse(mean(primary) > 0, 0, 1)) %>%
        filter(year == tail(preperiod_years, n=1)) %>%
        # need to make sure we don't select the treated unit (for the placebo tests)
        filter(control == 1, state != treated_unit) %>%
        select(state) %>%
        unlist(.) %>%
        as.numeric(.)
    
    # Create lookup table of state names to join onto data
    state_ids = data.frame(attr(data$state, 'label'))[,1]
    state_names = rownames(data.frame(attr(data$state, 'label')))
    state_names_df = data.frame(state = state_ids,
                                state_name = state_names,
                                stringsAsFactors=FALSE)
    
    # Prepare data for Synth
    # sink("sink")  # Silence output
    dataprep.out = data %>%
        mutate(state = as.numeric(unlist(state))) %>%
        left_join(state_names_df, by='state') %>%  # add state_name
        data.frame(.) %>%
        dataprep(foo = .,
                 predictors = predictors_,
                 time.predictors.prior = preperiod_years,
                 dependent = "ln_fat_pc",
                 unit.variable = "state",
                 unit.names.variable = "state_name",
                 time.variable = "year",
                 treatment.identifier = treated_unit,
                 controls.identifier = control_states,
                 time.optimize.ssr = preperiod_years,
                 time.plot = all_years
        )
    
    # Run Synth to create weighted control
    synth.out <- invisible(synth(data.prep.obj = dataprep.out, method = "BFGS"))
    # closeAllConnections()  # un-silence output
    
    return(list(
        dataprep.out = dataprep.out, 
        synth.out = synth.out
    ))
}


```

Can't use rural or urban speed limits as predictors because they do not change
across the control units.

```{r Apply Synth1, cache=Cache, results='hide'}
# Run Synth on preferred specification
predictors1 = c("college" , "beer" , "unemploy" , "ln_tvmt_pc" , "ln_precip", "snow32")

synth1 = apply_synth(data, predictors1)

synth.tables <- synth.tab(dataprep.res = synth1$dataprep.out,
                           synth.res = synth1$synth.out)

```

The Synth package calibrated the weights in a weighed average of the control
states to fit the covariates of the aggregate treated unit. The weights were
adjusted to minimize the sum of squared differences between the covariates of
the control units and the covariates of the aggreagte treated unit, in the years
before the treated unit was treated (`r min(preperiod_years)` - `r
max(preperiod_years)`).

The weights that minimize the sum of these distances are in Table \ref{tab:synth-weights}.

```{r synth-weights-table}
# Create table of Synth weights
synth.tables$tab.w %>%
    select(w.weights, unit.names) %>%
  kbl(caption = "Synthetic Control Weights\\label{tab:synth-weights}",
      col.names = c('Weight', 'State'),
      align = 'cc',
      row.names = F) %>%
  kable_styling(latex_options = "HOLD_position")

```

There are sums of squared differences for each covariate -- instead of just
taking a simple sum or average of these and picking control unit weights to
minimize that sum, the Synth package creates weights for the covariates to
create a weighted average of the sum of squared differences to minimize. The
covariates were first used to predict the pre-treatment outcome for the treated
unit, and covariates that have more power in predicting the outcome receive
large weights, and are more important in generating the synthetic control
weights. These weights are in Table \ref{tab:synth-cov-weights} and are used in
a weighted sum of the squared differences to find the optimal control unit
weights in constructing the synthetic control. We can that the weight is fairly
even across covariates except that precipitation has absorbed the weight that
snow would get -- probably because they are highly correlated.

```{r synth-var-weights-table}
# Create table of Synth variable weights
synth.tables$tab.v %>%
  kbl(caption = "Covariate Weights used sum of SSE's\\label{tab:synth-cov-weights}",
      col.names = c('Covariate Weights'),
      align = 'cc') %>%
  kable_styling(latex_options = "HOLD_position")

```



The weighted synthetic control results in covariate balance Table \ref{tab:cov-balance}
between the synthetic control and aggregate treated unit. We can see that the
balance is very close in the pre-treatment periods, except under snow, but that
was weighted very low in the synthetic control construction procedure because of
its strong correlation with precipitation.

```{r synth-cov-balance-table}
# Create variable balance table
synth.tables$tab.pred %>%
    data.frame() %>%
    select(Treated, Synthetic) %>%
  kbl(caption = "Covariate Balance in Pre-treatment Periods\\label{tab:cov-balance}",
      col.names = c('Treated', 'Synthetic'),
      align = 'ccc') %>%
  kable_styling(latex_options = "HOLD_position")

```




```{r Plot Aggregate Treated and Synth}
# Plot the TU and Synth control
path.plot(synth.res = synth1$synth.out,
           dataprep.res = synth1$dataprep.out,
           Ylab = "log(Traffic Fatalities per Captia)",
           Xlab = "year",
           Ylim = c(-2,-1),
           Legend = c("Aggregate Treatment State","Synthetic Control State"),
           Main = "Traffic Fatalities before and after Primary Seat Belt Laws",
           tr.intake = tail(preperiod_years, n=1)+1
           )

```



\newpage
## Part (c)
**Graphical interpretation and treatment significance.**

### ------ c.i
**Generate graphs plotting the gap between the TU and the synthetic control
group under both your preferred specification and a few other specifications
you tried.**

```{r Synth2 and Synth3, results='hide', cache=Cache}
# Run Synth on two more specifications
predictors2 = c("college" , "beer" , "unemploy" , "ln_tvmt_pc" , "ln_precip")
synth2 = apply_synth(data, predictors2)
predictors3 = c("college", "ln_precip")
synth3 = apply_synth(data, predictors3)

```



```{r Plot the gaps using Synth, eval=F}
# Plot the gaps of three specifications using Synth
gaps.plot(synth.res = synth3$synth.out,
           dataprep.res = synth3$dataprep.out,
           Ylab = "gap in log(Traffic Fatalities per Captia)",
           Xlab = "year",
           Main = "Difference in Traffic Fatalities (Aggregate Treated vs. Synthetic Control)",
           tr.intake = tail(preperiod_years, n=1)+1
           )

```


```{r Plot the gaps using ggplot}
# Retrieve gap between TU and synth control
get_gaps = function(synth_out){
    # Return vector of years and gaps between treated and synthetic control
    # negative gap ==> treated is less than control
    gaps = synth_out$dataprep.out$Y1plot -
        (synth_out$dataprep.out$Y0plot %*% synth_out$synth.out$solution.w)
    return(list(
        year = as.numeric(rownames(gaps)),
        gaps = as.numeric(gaps)
    ))
}

gaps1 = get_gaps(synth1)
gaps2 = get_gaps(synth2)
gaps3 = get_gaps(synth3)

gaps_df = rbind(
    data.frame(Year = gaps1$year, gaps = gaps1$gaps, specification = '1 (preferred)'),
    data.frame(Year = gaps2$year, gaps = gaps2$gaps, specification = '2'),
    data.frame(Year = gaps3$year, gaps = gaps3$gaps, specification = '3 (limited)')
)

# Plot the preferred specification in ggplot
gaps_df %>%
    filter(specification == '1 (preferred)') %>%
    ggplot(aes(x=Year, y=gaps)) +
    geom_line() +
    ylim(c(-0.22,0.1)) +
    ylab("gap in log(Traffic Fatalities per Captia)") +
    ggtitle('Difference in Traffic Fatalities (Aggregate Treated vs. Synthetic Control)',
            subtitle='Preferred Specification') +
    geom_hline(yintercept = 0, linetype = "dotted") +
    geom_vline(xintercept = tail(preperiod_years, n=1)+1, linetype = "dotted")

# Plot the gaps of three specifications using ggplot
gaps_df %>%
    ggplot(aes(x=as.integer(Year), y=gaps)) +
  geom_line(aes(color = factor(specification))) + 
  scale_color_manual(values = c("darkred", "steelblue", "orange")) +
    ylim(c(-0.22,0.5)) +
    xlab('Year') +
    ylab("gap in log(Traffic Fatalities per Captia)") +
    ggtitle('Difference in Traffic Fatalities (Aggregate Treated vs. Synthetic Control)',
            subtitle='Various Specifications of Predictor Variables') +
    labs(color = 'Specification') +
    geom_hline(yintercept = 0, linetype = "dotted") +
    geom_vline(xintercept = tail(preperiod_years, n=1)+1, linetype = "dotted") +
  theme(legend.position = c(0.87, 0.8))


```

```{r specification table}
predictors = unique(c(predictors1, predictors2, predictors3))
spec_df = data.frame(predictor=predictors)
for (i in 1:3) {
    # spec_df[, paste('Specification', i)] = 
    spec_df = spec_df %>%
        mutate("Specification {i}" := ifelse(predictor %in% eval(as.symbol(paste0('predictors', i))), 'x', ''))
}

spec_df %>%
    kbl(caption = paste("Predictors included in each specification\\label{tab:specifications}"),
        # col.names = c('State ID', 'log(fatatilities per captia)', 
        #               'Absolute difference between this state and treated'),
        row.names = F,
        align = 'cc') %>%
    kable_styling(latex_options = "HOLD_position")
```









### ------ c.ii
**Compare the graph plotting the gap between the TU and the synthetic control
group under your preferred specification with the graphs plotting the gap
between each control state and its “placebo” treatment. Do you conclude that the
treatment was significant? Why or why not?**


```{r Create placebo gaps, cache=Cache, results='hide'}
# Create list of control states
control_states = data %>%
    group_by(state) %>%
    mutate(control = ifelse(mean(primary) > 0, 0, 1)) %>%
    filter(year == tail(preperiod_years, n=1)) %>%
    filter(control == 1) %>%
    select(state) %>%
    unlist(.) %>%
    as.numeric(.)

# Create list of all years in sample
all_years = data %>%
    arrange(year) %>%
    select(year) %>%
    unique() %>%
    unlist(.) %>%
    as.numeric(.)

# Dataframe to store gaps results
gaps_df2 = data.frame(Year = gaps1$year,
                      gaps = gaps1$gaps,
                      unit = 99,
                      color = 'black')

# Create a progress bar ... this might take a while
progress = progress_bar$new(
    format = " State :id [:bar] :percent eta: :eta",
    total = length(control_states),
    clear = FALSE, width= 60)

# Create placebo synthetic controls for all control states
for (state_id in control_states){
    progress$tick(tokens = list(id = state_id))
    # Run Synth for control state state_id, get placebo gaps
    gaps_temp = invisible(get_gaps(apply_synth(data, 
                                     predictors1, 
                                     treated_unit = state_id)))
    # Add gaps to dataframe
    temp_df = data.frame(Year = gaps_temp$year,
                         gaps = gaps_temp$gaps,
                         unit = state_id,
                         color = 'grey')
    gaps_df2 = rbind(gaps_df2, temp_df)
}

```


```{r Plot all the gaps}
# Plot all the gaps (actual treatment estimate and control placebos)
gaps_df2 %>%
    ggplot(aes(x=as.integer(Year), y=gaps, group=unit)) +
    geom_line(aes(color = color)) +
    scale_color_identity() + scale_linetype_identity() +
    ylim(c(min(gaps_df2$gaps), max(gaps_df2$gaps)+0.1)) +
    xlab('Year') +
    ylab("gap in log(Traffic Fatalities per Captia)") +
    ggtitle('Difference in Traffic Fatalities between States and their Synthetic Control',
            subtitle='Actual and Placebo Tests') +
    geom_hline(yintercept = 0, linetype = "dotted") +
    geom_vline(xintercept = tail(preperiod_years, n=1)+1, linetype = "dotted") +
    theme(legend.position = c(0.75, 1), legend.title=element_blank()) +
    scale_color_manual(labels = c("Treated vs Synthetic Control", 
                                  "Control vs. Placebo Synthetic Control"), 
                       values = c("black", "grey"))


```



Based on this graph alone (before seeing the placebo test), we might conclude that there was indeed a treatment effect of the policy. However, without more context (provided in 2ciii), we can't say.







### ------ c.iii
**Create a graph of the post-treatment/pre-treatment prediction ratios of the
Mean Squared Prediction Errors (MSPE) for the actual and “placebo” treatment
gaps in (ii). [See Abadie et al. for an example]. Do you conclude that the
treatment was significant? Why or why not?**

```{r MSPE ratio text}
ratio_text = "Histogram of the ratios of Mean Squared Prediction Errors of the synthetic controls in the post-treatment compared to the pre-treatment years.\\label{fig:ratios}"
```


```{r historgram of MSPE ratios, fig.width=8, fig.cap=ratio_text, message=F, warning=F}
MSPE = function(df, treatment_year) {
    pretreat = filter(df, year < treatment_year)
    posttreat = filter(df, year1 >= treatment_year)
    SSEpre = sum(pretreat$gaps^2)
    SSEpost = sum(posttreat$gaps^2)
    return(SSEpost / SSEpre)
}

MSPE_ratios = gaps_df2 %>%
    mutate(post = ifelse(Year > max(preperiod_years), 'post', 'pre')) %>%
    group_by(unit, post) %>%
    summarize(MSPE = mean(gaps^2)) %>%
    pivot_wider(names_from = post, values_from = MSPE) %>%
    mutate(MSPE_post_pre = post / pre) %>%
    data.frame() %>%
    mutate(rank = dense_rank(desc(MSPE_post_pre)))

TU_ratio = MSPE_ratios %>%
    filter(unit == 99) %>% 
    select(contains('MSPE')) %>% unlist %>% unname

TU_rank = MSPE_ratios %>%
    filter(unit == 99) %>% 
    select(contains('rank')) %>% unlist %>% unname

total_ratios = nrow(MSPE_ratios)

MSPE_ratios %>%
    ggplot(aes(x = MSPE_post_pre)) +
    geom_histogram() +
    ylab(paste('Count (of', length(control_states) + 1, 'synthetic control comparisons)')) +
    xlab('post/pre MSPE ratio') +
    geom_vline(aes(xintercept=TU_ratio, color='red')) +
    annotate(geom = "text", x = TU_ratio, y = 10, 
             label = paste(" MSPE post/pre ratio for actual Treatment Unit: ", round(TU_ratio,1)), 
             hjust = "left", color = 'red') +
    theme(legend.position = "none")
    
```


From the distribution of MSPEs, we see that our treatment cannot be ruled out as mere chance. It is not even more than one standard deviation from the mean MSPE of placebo tests.

Since we have 30 control states, this gives us a total of 31 synthetic control
comparisons. We can calculate the MSPE of the synthetic control compared to the
Treatment Unit separately for the pre-treatment years and post-treatement years.
Taking the post-pre ratio of MSPE gives us a sense of how divergent the Treatment
Unit is from its synthetic control. We can do this for all 31 synthetic control
experiments (30 placebos using the 30 controls, and the actual Treatment Unit).

In \ref{fig:ratios}, we can see that the post/pre MSPE ratio falls somewhat on
the right side of the distribution. We can calculate a p-value as the
probability that the result we got was not just from random chance. This is the
number of MSPE ratios that are equal to or greater than the Treatment Unit's
MSPE ratio, divided by the total number of ratios. The p-value for our actual
synthetic control estimate is `r round(TU_rank/total_ratios, 3)`, so we cannot
reject the null hypothesis that the decrease in fatalities is merely from random
variation in the data.

\[
\text{p-value} =
\frac{\text{\# of MSPE ratios at or above the TU}}{\text{total \# of MSPE ratios}} =
\frac{`r TU_rank`}{`r total_ratios`} =
`r round(TU_rank/total_ratios, 3)`
\]






















\newpage
## Part (d)
**How do your synthetic control results compare to your fixed effects results
from Question (3) in the last problem set? Interpret any differences.**





















\newpage
# Appendix A: R Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```





















