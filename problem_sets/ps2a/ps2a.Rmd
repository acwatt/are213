---
title: "ARE 213 PS 2a"
author: "S. Sung, H. Husain, T. Woolley, A. Watt"
email: aaron@acwatt.net
date: "2021-11-08"
output:
  pdf_document:
    toc: true
    toc_depth: 2
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{amsmath}
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'ARE_213_PS2a.pdf')) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps2a for ARE 213

Notes: Need to ps2a.R from ps2a folder (without Rstudio being started first)
       if Rstudio is already started, the working directory will not be set
       to the ps2a/ folder
       
       \usepackage{dcolumn}: dcolumn is needed in latex compilation 
-->

```{r Settings, echo=FALSE}
# stargazer table type (html, latex, or text)
# Change to latex when outputting to PDF, html when outputting to html
table_type = "text"
```




# Packages
```{r packages, results='hide', message=FALSE, echo=T}
library(tidyverse)
library(haven)
library(plm)
library(lmtest)
library(sandwich)
library(stargazer)
library(ggplot2)
library(gridExtra)
library(grid)
library(gtable)
library(tinytex)
library(fastDummies)
```



\newpage
<!--=========================================================================-->
# Problem 1
<!--=========================================================================-->
**Question 10.3 from Wooldridge: For $T = 2$ consider the standard unobserved
effects model:**

\begin{equation}
y_{it} = \alpha + x_{it}\beta + c_i + u_{it}
\label{eq:1}
\end{equation}

\def\bfe{\hat\beta_{FE}}
\def\bfd{\hat\beta_{FD}}
\def\dxit{\Delta X_{it}}
\def\dxi{\Delta X_i}
\def\uxit{\ddot X_{it}}
\def\dyit{\Delta y_{it}}
\def\dyi{\Delta y_i}
\def\uyit{\ddot y_{it}}
\def\sumi{\sum\limits_i}
\def\sumt{\sum\limits_t}
\def\sumit{\sumi\sumt}
\def\lp{\left(}
\def\rp{\right)}

**Let $\bfe$ and $\bfd$ represent the fixed effects and first
differences estimators respectively.**

## Part (a)
**Show that $\bfe$ and $\bfd$ are numerically identical. Hint: it may
be easier to write $\bfe$ as the “within estimator” rather than the
fixed effects estimator.**

\begin{align*}
\intertext{Writing $\bfe$ as the within estimator, $\bfe$ and $\bfd$ are given by}
\bfd &= \left(\Delta X'\Delta X\right)^{-1}\left(\Delta X'\Delta y\right) 
\qquad\text{and}\qquad 
\bfe = \left(\ddot X'\ddot X\right)^{-1}\left(\ddot X'\ddot y\right)\\
\intertext{Expanding the inner products, we have}
\bfd &= \left(\sumit\dxit'\dxit\right)^{-1}\left(\sumit\dxit'\dyit\right)
\intertext{and} 
\bfe &= \left(\sumit\uxit'\uxit\right)^{-1}\left(\sumit\uxit'\uyit\right)\\
\intertext{Since there are only two periods, $\bfd$ simplifies to}
\bfd &= \left(\sumi\dxi'\dxi\right)^{-1}\left(\sumi\dxi'\dyi\right)  \\
\intertext{where} \dxi &\equiv X_{i2} - X_{i1} 
    \qquad\text{and}\qquad 
     \dyi \equiv y_{i2} - y_{i1} \\
\intertext{Now we note that}
\ddot X_{i1} &= X_{i1} - \frac{1}{2}(X_{i1}+X_{i2}) 
    = \frac{1}{2}(X_{i1}-X_{i2}) = -\frac{1}{2}\dxi
\intertext{and similiarly}
\ddot X_{i2} &= \frac{1}{2}\dxi, \quad
    \ddot y_{i1} = -\frac{1}{2}\dyi \quad
    \ddot y_{i2} = \frac{1}{2}\dyi \\
\intertext{Then, $\bfe$ becomes}
\bfe &= \left(\sumit\uxit'\uxit\right)^{-1}\left(\sumit\uxit'\uyit\right)\\
    &= \left(\sumi\frac{1}{4}\dxi'\dxi + \frac{1}{4}\dxi'\dxi \right)^{-1}
    \left(\sumi\frac{1}{4}\dxi'\dyi + \frac{1}{4}\dxi'\dyi\right)\\
    &= \left(\frac{1}{2}\sumi\dxi'\dxi\right)^{-1}
    \left(\frac{1}{2}\sumi\dxi'\dyi\right)\\
    &= \left(\sumi\dxi'\dxi\right)^{-1}
    \left(\sumi\dxi'\dyi\right)\\
    &= \bfd
\end{align*}

So $\bfe$ and $\bfd$ are numerically identical.



\newpage
## Part (b)
**Show that the standard errors of $\bfe$ and $\bfd$ are numerically identical.
If you wish, you may assume that $x_{it}$ is a scalar (i.e. there is only one
regressor) and ignore any degree of freedom corrections. You are not clustering
the standard errors in this problem.**

\def\sufe{\hat\sigma_{u,FE}^2}
\def\sufd{\hat\sigma_{u,FD}^2}
\def\dui{\widehat{\Delta u}_i}
\def\uuit{\widehat{\ddot u}_{it}}
\def\bhat{\hat\beta}

\begin{align*}
\intertext{The standard errors are estimates of the square root of the asymptotic variances of our estimators, so WLOG, we can compare the asymptotic variances. The asymptotic variances of our estimators are}
\widehat{Avar(\bfe)} &= \sufe \left(\ddot X'\ddot X\right)^{-1}
    \qquad\text{and}\qquad 
    \widehat{Avar(\bfe)} = \sufd \left(\Delta X'\Delta X\right)^{-1}
\intertext{where $\sufe$ and $\sufd$ are estimated from the residuals of the corresponding regressions and using the correct degrees of freedom:}
\sufe &= \frac{\sumit\uuit^2}{N(T-1)-K} = \frac{\sumit\uuit^2}{N-K}
    \qquad\text{and}\qquad
    \sufd = \frac{\sumi\dui^2}{N(T-1)-K} = \frac{\sumi\dui^2}{N-K}
\intertext{Let $\bhat:=\bfd=\bfe$. Then, from part (a), we can find the relationship between $\dui$ and $\uuit$:}
\uuit^2 &= (\uyit - \uxit\bhat)^2 \\
    &= \lp(-1)^t \lp\frac{1}{2}\dyi - \frac{1}{2}\dxi\bhat \rp\rp^2 \\
    &= \frac{1}{4}\lp \dyi - \dxi\bhat \rp ^2 \\
    &= \frac{1}{4}\dui^2
\intertext{So the estimated error variances are related by}
\sufe &= \frac{\sumit\uuit^2}{N-K} \\
    &= \frac{\sumit\frac{1}{4}\dui^2}{N-K} \\
    &= \frac{\sumi\frac{1}{2}\dui^2}{N-K} \\
    &= \frac{1}{2}\frac{\sumi\dui^2}{N-K} \\
    &= \frac{1}{2}\sufd \\
\intertext{We know from part (a) that}
\lp\sumit\uxit'\uxit\rp^{-1} &= \lp\frac{1}{2}\sumi\dxi'\dxi\rp^{-1} \\
    &= 2\lp \sumi\dxi'\dxi \rp^{-1}
\intertext{And putting all these together, we have}
\widehat{Avar(\bfe)} &= \sufe \lp \ddot X'\ddot X \rp^{-1} \\
    &= \frac{1}{2}\sufd 2\lp \sumi\dxi'\dxi \rp^{-1} \\
    &= \sufd \lp \sumi\dxi'\dxi \rp^{-1} \\
    &= \widehat{Avar(\bfe)}
\end{align*}
Because the estimates of the asymptotic variances are equal, the standard errors (the square roots) will be equal.





\newpage
<!--=========================================================================-->
# Problem 2
<!--=========================================================================-->
**Question 21-3 from Cameron-Trivedi (enhanced): Consider the fixed
effects, two-way error component panel data model:**

\begin{equation}
y_{it} = \alpha + x_{it}\beta + \mu_i + \lambda_t + \epsilon_{it}
\label{eq:2}
\end{equation}

## Part (a)
**Show that the fixed effects estimator of $\beta$ can be obtained by applying
two within (one-way) transformations on this model. The first is the within
transformation ignoring the time effects followed by the within transformation
ignoring the individual effects. Assume the panel is balanced. (Hint: it may be
easier to analyze the fixed effects regression using partitioned regression.)**

\def\xddot{\ddot x_{it}}
\def\yddot{\ddot y_{it}}
\def\eddot{\ddot e_{it}}

\def\xdddot{\dddot x_{it}}
\def\ydddot{\dddot y_{it}}
\def\edddot{\dddot e_{it}}

\def\sumi{\sum\limits_i}
\def\sumt{\sum\limits_t}
\def\sumit{\sumi\sumt}

\begin{align*}
\intertext{We want to show that} 
\ydddot = \xdddot\beta + \edddot
\intertext{Ignoring time effects, we get}
\yddot = y_{it} - \overline{y_{i}} = y_{it} - \frac{1}{T}\sumt y_{it} \\
\implies \yddot = y_{it} - \frac{1}{T}\sumt \left(\alpha + x_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it} \right)
\intertext{Applying the second within transformation, we get that}
\ydddot = \yddot - \overline{\yddot} \\
= \yddot - \frac{1}{N} \sumi \yddot \\
= y_{it} - \overline{y_i} - \overline{y_t} + \overline{y} \\
= \alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it} \\
- \left(\frac{1}{T} \sumt (\alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it})\right) \\
- \left(\frac{1}{N} \sumi (\alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it})\right) \\
+ \left(\frac{1}{NT} \sumit (\alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it})\right) \\ 
= \beta \left(x_{it} - \overline{x_{i}} - \overline{x_{t}} + \overline{x}\right) + \epsilon_{it} - \overline{\epsilon_{i}} - \overline{\epsilon_{t}} + \overline{\epsilon} \intertext{Since} 
\xdddot = x_{it} - \overline{x_{i}} - \overline{x_{t}} + \overline{x} 
\intertext{and}
\edddot = e_{it} - \overline{e_{i}} - \overline{e_{t}} + \overline{e}
\intertext{We get that}
\ydddot = \xdddot\beta + \edddot
\end{align*}


## Part (b)
**Show that the order of the transformations is unimportant. Give an intuitive
explanation for why.**

\begin{align*}
\intertext{Reversing the order, we can show that we get the same result}
\intertext{Again, we want to show that} 
\ydddot = \xdddot\beta + \edddot
\intertext{Ignoring individual effects, we get}
\yddot = y_{it} - \overline{y_{t}} = y_{it} - \frac{1}{N}\sumi y_{it} \\
\implies \yddot = y_{it} - \frac{1}{N}\sumi \left(\alpha + x_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it} \right)
\intertext{Applying the second within transformation, we get that}
\ydddot = \yddot - \overline{\yddot} \\
= \yddot - \frac{1}{T} \sumi \yddot \\
= y_{it} - \overline{y_t} - \overline{y_i} + \overline{y} \\
= \alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it} \\
- \left(\frac{1}{N} \sumi (\alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it})\right) \\
- \left(\frac{1}{T} \sumt (\alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it})\right) \\
+ \left(\frac{1}{NT} \sumit (\alpha + X_{it}\beta + \mu_{i} + \lambda_{t} + \epsilon_{it})\right) \\ 
= \beta \left(x_{it} - \overline{x_{t}} - \overline{x_{i}} + \overline{x}\right) + \epsilon_{it} - \overline{\epsilon_{t}} - \overline{\epsilon_{i}} + \overline{\epsilon} \intertext{Since} 
\xdddot = x_{it} - \overline{x_{t}} - \overline{x_{i}} + \overline{x} 
\intertext{and}
\edddot = e_{it} - \overline{e_{t}} - \overline{e_{i}} + \overline{e}
\intertext{We get that}
\ydddot = \xdddot\beta + \edddot
\intertext{Intuitively, the order of the transformations is unimportant because in the end, were still manage to difference out the individual and time effects. There is nothing particular to individual or time effects that would warrant removal in any particular order.}
\end{align*}

## Part (c)
**Does your answer to part (a) change if the panel becomes unbalanced (i.e.,
contains different numbers of observations for each individual $i$). Why or why not?**




\newpage
<!--=========================================================================-->
# Problem 3
<!--=========================================================================-->
**We now begin with an actual analysis of the data. The goal here is to
determine what effect, if any, primary belt laws have on the log of traffic
fatalities per capita (we log the LHS variable because we believe the effect of
safety belt laws should be proportional to the overall level of fatalities per
capita).**


```{r Load Data, results='hide', eval=T}
data = read_dta('traffic_safety2.dta') %>%
    filter(state != 99) %>%
    mutate(fatal_per_cap = fatalities / population,
           vmt_per_cap = totalvmt/population)
```


## Part (a)
**Run pooled bivariate OLS. Interpret. Add year fixed effects. Interpret. Add all covariates that you believe are appropriate. Think carefully about which covariates should be log transformed and which should enter in levels. What happens when you add these covariates? Why?**

```{r Bivariate OLS}
df <- data %>%
        mutate(fat_pc = fatalities/population,
               ln_fat_pc = log(fat_pc),
               ln_tvmt_pc = log(totalvmt/population))

reg3a_1 <- lm(ln_fat_pc ~ primary, data = df)
summary(reg3a_1)
stargazer(reg3a_1, type = "text")
```

```{r OLS with year fixed effects}
reg3a_2 <- lm(ln_fat_pc ~ primary + factor(year), data = df)
stargazer(reg3a_2, type = "text", no.space = TRUE, omit.stat=c("f", "ser"))

df.p <- pdata.frame(df, index = c("state", "year")) #declaring panel dataset
reg3a_2_p <- plm(ln_fat_pc ~ primary, data = df.p, effect = "time", model = "within") #checking reg3a_2
stargazer(reg3a_2_p, type = "text", no.space = TRUE, omit.stat=c("f", "ser"))
```



total vehicle miles traveled per capita: logged because, after using per capita
variables, we think the increase in fatalities per capita would be proportional
to a percentage point increase in vehicle miles traveled per capita, not
absolute levels, since the percentage point increase is a better measure of
deviation from the norm, and drivers are more likely to adjust poorly to a
deviation from the norm than an absolute increase in vehicle miles traveled
(since an absolute increase may not be very differnt from the norm in states
that have relatively large average miles traveled.)

Not inlcuding population because we are using per-capita variables

Log precip because we car about percentage-point deviation from the norm. Don't log snow because it has zeros.

When we add the covariates, the estimated effect on primary switches from significantly negative to insignificantly positive. Since we haven't yet controlled for state-level fixed effects, this could be from trying to compare states that have similar observable covariates that adopt primary safety belt laws at different times, but may have significant reasons for adopting when they did, and thus may have significant unobserved factors that are correlated with the primary safety belt law being in place.

```{r OLS with controls and year fixed effects}
# Bivariate with lm
reg_a_bivariate_lm = lm(log(fatal_per_cap) ~ primary , data = data)
# Bivariate with plm
reg_a_bivariate_plm = plm(log(fatal_per_cap) ~ primary, 
                       data = data,
                       model = "pooling")
# FE with lm
reg_a_yfe_lm = lm(log(fatal_per_cap) ~ primary + factor(year), data = data)
# FE with plm
reg_a_yfe_plm = plm(log(fatal_per_cap) ~ primary, 
                data = data,
                     index = c("state", "year"),  # order matters: unit-var, time-var
                     model = "within",
                     effect = "time")  # only do time
# FE + covars with lm
reg_a_full_lm = lm(log(fatal_per_cap) ~ primary + factor(year) + college + beer
                   + secondary + unemploy + log(vmt_per_cap) + log(precip) + snow32
                   + log(rural_speed) + log(urban_speed), data = data)
# FE + covars with plm
reg_a_full_plm = plm(log(fatal_per_cap) ~ primary + factor(year) + college + beer
                     + secondary + unemploy + log(vmt_per_cap) + log(precip) + snow32
                     + log(rural_speed) + log(urban_speed), 
                     data = data,
                     index = c("state", "year"),  # order matters: unit-var, time-var
                     model = "within",
                     effect = "time")  # only do time


# Checking in-state variation in rural_speed
# d = data %>% group_by(state) %>% mutate(rural_speed_dev = abs(rural_speed - mean(rural_speed)), rural_speed_dev_mean = mean(rural_speed_dev))
# plot(d$state, d$rural_speed_dev_mean)
```

```{r 3a table Aaron, results='asis'}
stargazer(reg_a_bivariate_plm, reg_a_yfe_plm, reg_a_full_plm, type='text')
```

```{r 3a table, results='asis'}
reg3a_3 <- plm(ln_fat_pc ~ primary + secondary + college + beer + unemploy 
               + ln_tvmt_pc + precip + snow32 + rural_speed + urban_speed, 
               data = df.p, effect = "time", model = "within")
stargazer(reg3a_3, type = "text", no.space = TRUE, omit.stat=c("f", "ser"))

reg3a_3.1 <- plm(ln_fat_pc ~ primary + college + beer + unemploy 
               + ln_tvmt_pc + precip + snow32 + rural_speed + urban_speed, 
               data = df.p, effect = "time", model = "within")
stargazer(reg3a_3.1, type = "text", no.space = TRUE, omit.stat=c("f", "ser"))

# with state fixed effects as well
reg3a_3.2 <- plm(ln_fat_pc ~ primary + secondary + college + beer + unemploy 
               + ln_tvmt_pc + precip + snow32 + rural_speed + urban_speed + factor(state), 
               data = df.p, effect = "time", model = "within")
stargazer(reg3a_3.2, type = "text", no.space = TRUE, omit.stat=c("f", "ser"))

reg3a_3.3 <- plm(ln_fat_pc ~ primary + college + beer + unemploy 
               + ln_tvmt_pc + precip + snow32 + rural_speed + urban_speed + factor(state), 
               data = df.p, effect = "time", model = "within")
stargazer(reg3a_3.3, type = "text", no.space = TRUE, omit.stat=c("f", "ser"))
```

We included covariates that we believe to be correlated (both statistically and theoretically) with both the outcome (log fatalities per capita) and the treatment variable (primary). This includes weather variables, education, beer, unemployment, speed limits, and tvmt. Since tvmt is a level variable, we thought it best to divide it by population and take its log. We included "secondary" in one of the regressions to identify the marginal effect of having a primary seatbelt law in addition to the secondary one where that is already in place. 

Including these covariates drastically changes our point estimate. When the covariates other than secondary are include, the effect essentially disappears though remains negative. This is due to the fact that variation in fatalities per capita that we previously attributed to "primary" in the OLS regression without covariates is actually attributable to these other characteristics. Furthermore, once secondary is included, the effect remains insignificant but turns positive. If this is actually a true zero, then this could be because the secondary policy already prevents fatalities. If this is a true positive (we are less convinced), then it could be that the primary policy causes people to take less responsibility of one's own actions and depend on law enforcement. Laws can cause rebellion from teens and also unintentiolly shift responsibility from the citizen to the government.

In addition, it is worth noting that when adding state fixed effects, the coefficient on primary becomes negative and significant, even with and without controling for "secondary." In fact, it is even more negative when controlling for "secondary." This, to us, seems to identify the treatment effect since it controls for any intrinsic state characteristics that were not included and could have cause omitted variable bias.


## Part (b)
**Ignore omitted variables bias issues for the moment. Do you think the standard
errors from above are right? Compute the Huber-White heteroskedasticity robust
standard errors (e.g., ", robust"). Do they change much? Compute the clustered
standard errors that are robust to within-state correlation (e.g., ",
cluster(state)"). Do this using both the "canned" command and manually using the
formulas we learned in class. Do the standard errors change much? Are you
surprised? Interpret.**

Standard errors from specifications in part (a) is likely incorrect given we have potential correlation across observations from same state.

```{r 3b Robust SE}
reg3b_1_robust <- coeftest(reg3a_1, vcov = vcovHC(reg3a_1, method = "white1", type = "HC0"))
reg3b_2_robust <- coeftest(reg3a_2_p, vcov = vcovHC(reg3a_2_p, method = "white1", type = "HC0"))
reg3b_3_robust <- coeftest(reg3a_3, vcov = vcovHC(reg3a_3, method = "white1", type = "HC0"))

stargazer(reg3a_1, reg3b_1_robust, reg3a_2_p, reg3b_2_robust, reg3a_3, reg3b_3_robust,
          title = "Regression with Robust SE",
          dep.var.caption = "Log(Fatality per Population)",
          dep.var.labels.include = FALSE, model.names = FALSE,
          column.labels = c("Biv", "Biv(Robust)", "Yr FE", "Yr FE(Robust)", "Ctr/Yr FE", "Ctr/Yr FE(Robust)"),
          keep = "primary",
          font.size = "footnotesize", column.sep.width = "1pt", no.space = TRUE, omit.stat=c("f", "ser"),
          type = "text", digits = 4)
```

We observe that for simple bivariate regression and spepcification with only year fixed effects, robust standards are slightly larger. In model with controls and time fixed effects, robust standard errors are slightly smaller.

```{r 3b Robust SE (Canned)}
reg3b_1_cluster <- coeftest(reg3a_1, vcov = vcovCL, cluster = ~state)
reg3b_2_cluster <- coeftest(reg3a_2_p, vcov = vcovHC(reg3a_2_p, type = "HC0", cluster = "group"))
reg3b_3_cluster <- coeftest(reg3a_3, vcov = vcovHC(reg3a_3, type = "HC0", cluster = "group"))

stargazer(reg3a_1, reg3b_1_cluster, reg3a_2_p, reg3b_2_cluster, reg3a_3, reg3b_3_cluster,
          title = "Regression with Clustered SE",
          dep.var.caption = "Log(Fatality per Population)",
          dep.var.labels.include = FALSE, model.names = FALSE,
          column.labels = c("Biv", "Biv(Cluster)", "Yr FE", "Yr FE(Cluster)", "Ctr/Yr FE", "Ctr/Yr FE(Cluster)"),
          keep = "primary",
          font.size = "footnotesize", column.sep.width = "1pt", no.space = TRUE, omit.stat=c("f", "ser"),
          type = "text", digits = 4)
```


```{r 3b Robust SE (Manual Calculation)}
# Manually Calculate Robust SE for Bivariate OLS
Y_a <- df[,"ln_fat_pc"] %>%
          as.matrix() #1104 x 1

X_a <- df[,"primary"] %>% 
          mutate(cons = 1) %>%
          select(cons, primary) %>%
          as.matrix() #1104 x 2

XX_inv <- solve(t(X_a) %*% X_a) #2 x 2

B_a = XX_inv %*% (t(X_a)%*%Y_a) #2 x 1 # This matches reg3a_1$coefficients

Resid_a <- Y_a - (X_a %*% B_a) #1104 x 1 # This matches reg3a_1$residuals

v2_a <- diag(diag(Resid_a %*% t(Resid_a))) #1104 x 1104

vcov_cluster <- (XX_inv) %*% (t(X_a) %*% v2_a %*% X_a) %*% XX_inv

vcov_cluster[2,2]^0.5
```


We observe that clustered SE are significantly higher than the original as well
as robust SE. This suggest that correlation within state over time is seriously
biasing our conventional standard errors.

From the below plots of the residuals, we can see that the variance of the
residuals is not constant along many of our covariates -- especially speed
limits, unemployment rates, and snow levels. Heteroskedasticity is an issue we
need to correct for.

```{r 3.b heteroskedasticity check, echo=F}
grid.arrange(
qplot(data$college, resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(data$beer, resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(data$unemploy, resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(log(data$precip), resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(data$snow32, resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(log(data$rural_speed), resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(log(data$urban_speed), resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
qplot(log(data$vmt_per_cap), resid(reg_a_bivariate_plm), ylab="residuals", alpha=I(0.1)),
nrow = 4, heights=rep(12,4))


#     p1 <- qplot(mpg, wt, data = mtcars, colour = cyl)
# p2 <- qplot(mpg, data = mtcars) + ggtitle("title")
# p3 <- qplot(mpg, data = mtcars, geom = "dotplot")
# p4 <-
#   p1 + facet_wrap( ~ carb, nrow = 1) + theme(legend.position = "none") +
#   ggtitle("facetted plot")
# grid.arrange(p1,p2)
```

```{r 3.b bivariate robust SE (manual)}
# by hand
Sigma = diag(resid(reg_a_bivariate_plm)^2)
X = model.matrix(~ primary, data = data)
XpX = t(X)%*%X
XpXinv = solve(XpX)
XpSigmaX = t(X)%*%Sigma%*%X
var_b = XpXinv %*% XpSigmaX %*% XpXinv
SE_3b_1 = sqrt(diag(var_b))
SE_3b_1

# canned
coeftest(reg_a_bivariate_lm, vcov = vcovHC(reg_a_bivariate_lm, method="white1", type="HC0"))
```



```{r 3.b year FE robust SE (manual)}
# by hand
d_temp = data %>%
    mutate(resids = reg_a_yfe_lm$residuals) %>%
    dummy_cols(., select_columns = 'year') %>%
    select(primary, resids, state, contains('year_'))
k = ncol(d_temp) - 2
meat = matrix(0, nrow=k, ncol=k)
# sum over all years
# for each year, subset just that year and calc X'(ee')X
for (s in unique(data$state)) {
    X = as.matrix(d_temp %>% filter(state==s) %>% select(-state, -resids))
    e = as.matrix(d_temp %>% filter(state==s) %>% select(resids))
    meat = meat + t(X)%*% (e%*%t(e)) %*% X
}

# X'X on full data
X = as.matrix(d_temp %>% select(-state, -resids))
XpX = t(X)%*%X
XpXinv = solve(t(X)%*%X)
N = length(unique(data$state))
T_ = length(unique(data$year))
var_b = XpXinv %*% meat %*% XpXinv * (N*T_ - 1) * T_ / ((N*T_ - k)*(T_ - 1))
SE_b = sqrt(diag(var_b))
rm(d_temp)
# Cluster robust standard errors
SE_b3_2[1]
# Canned
coeftest(reg_a_yfe_lm, vcov = vcovCL, cluster = ~state)[[2,2]]
```


```{r 3.b year FE + covariates robust SE (manual)}
# by hand
d_temp = data %>%
    mutate(resids = reg_a_yfe_lm$residuals) %>%
    dummy_cols(., select_columns = 'year') %>%
    select(primary, resids, state, contains('year_'))
k = ncol(d_temp) - 2
meat = matrix(0, nrow=k, ncol=k)
# sum over all years
# for each year, subset just that year and calc X'(ee')X
for (s in unique(data$state)) {
    X = as.matrix(d_temp %>% filter(state==s) %>% select(-state, -resids))
    e = as.matrix(d_temp %>% filter(state==s) %>% select(resids))
    meat = meat + t(X)%*% (e%*%t(e)) %*% X
}

# X'X on full data
X = as.matrix(d_temp %>% select(-state, -resids))
XpX = t(X)%*%X
XpXinv = solve(t(X)%*%X)
N = length(unique(data$state))
T_ = length(unique(data$year))
var_b = XpXinv %*% meat %*% XpXinv * (N*T_ - 1) * T_ / ((N*T_ - k)*(T_ - 1))
SE_3b_3 = sqrt(diag(var_b))
rm(d_temp)
# Cluster robust standard errors
SE_3b_3[1]

# Canned
coeftest(reg_a_full_lm, vcov = vcovCL, cluster = ~state)[[2,2]]
```


## Part (c)
**Compute the between estimator, both with and without covariates. Under what
conditions will this give an unbiased estimate of the effect of primary seat
belt laws on fatalities per capita? Do you believe those conditions are met? Are
you concerned about the standard errors in this case?**

In order for between estimators to be unbiased, individual state time-in-varying effects, which now rests within error term, should not correlated with included 'primary' or other co-variates. I do not think this "uncorrelated effects" condition would hold, given each states' time-invariant characteristics regarding average driving styles or fatality rate would likely impact whether state implements primary and secondary laws as well as driving conditions captured by covariates. With the between estimator, I am worried about SE, given we are down to 48 observations, one for each state, by averaging each state's observation over time.


```{r}
df_within <- df %>% select(-c("state", "year"))
df_within <- aggregate(df_within, list(df$state), mean)

sapply(df_within, typeof)
is.numeric(df_within$college)

reg_3c_btw_nocov <- plm(ln_fat_pc ~ primary, 
                            data = df_within, 
                            model = "between")


reg_3c_btw_nocov

# Is between estimator same as simply including time fixed effects
#reg_3c_time <- plm(ln_fat_pc ~ primary, 
#                              data = df, 
#                              index = c("state", "year"),
#                              model = "between",
#                              effects = "time")

reg_3c_btw_cov <- plm(ln_fat_pc ~ primary + secondary + beer + as.numeric(college) + unemploy
                            + ln_tvmt_pc + precip + snow32 + rural_speed + urban_speed,
                            data = df_within, 
                            model = "between")

reg_3c_btw_cov
```


## Part (d)
**Compute the Random Effects estimator (including covariates). Under what
conditions will this give an unbiased estimate of the effect of primary seat
belt laws on fatalities per capita? What are its advantages or disadvantages as
compared to pooled OLS?**

```{r}
reg_3d_btw_nocov <- plm(ln_fat_pc ~ primary + secondary + college + beer + unemploy
                            + ln_tvmt_pc + precip + snow32 + rural_speed + urban_speed,
                            data = df, 
                            model = "random")

reg_3d_btw_nocov
```


## Part (e)
**Do you think the standard errors from RE are right? Compute the clustered
standard errors. Are they substantially different? If so, why? (i.e., what
assumption(s) are being violated?)**



## Part (f)
**Compute the FE estimator using only primary and year fixed effects as the
covariates. Compute the normal standard errors and the clustered standard
errors. If they are different, why?**



## Part (g)
**Add the same range of covariates to the FE estimator that you did to the OLS
estimator. Are the FE estimates more or less stable than the OLS estimates?
Why?**



## Part (h)
**Estimate a first-differences estimator, a 5-year differences estimator, and a
long differences estimator, including year fixed effects (when feasible) and the
appropriate covariates in each case. Briefly describe the pattern that emerges
from the three differencing estimates. Where does the FE estimate fall in this
pattern? Are you surprised?**



## Part (i)
**Make the case that the first-differences estimate is superior to the 5-year or
long differences estimates.**



## Part (j)
**Make the case that the 5-year or long differences estimates are superior to
the first-differences estimate.**
















