---
title: "ARE 213 PS 1b"
author: "S. Sung, H. Husain, T. Woolley, A. Watt"
email: aaron@acwatt.net
date: "2021-10-18"
output:
  html_document:
    toc: true
    toc_depth: 2
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'ARE_213_PS1b.html')) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps1b for ARE 213

Notes: Need to ps1b.R from os1b folder (without Rstudio being started first)
       if Rstudio is already started, the working directory will not be set
       to the ps1b/ folder
-->





# Packages
```{r packages, results='hide', message=FALSE}
# install.packages("pacman")
# install.packages("tidyverse")
# install.packages("plm")
# install.packages('foreign')
# install.packages('stargazer')
# install.packages("finalfit")
# install.packages("glmnet")
# install.packages("jtools")
# install.packages("Hmisc")
library(tidyverse)
library(foreign)
# library(xtable)
library(stargazer)
library(finalfit)
library(glmnet)
library(jtools)  # summ() for regression summaries
```

# Data cleaning from PS 1a
```{r data_cleaning}
data = read.dta('ps1.dta')
missing_codes = read.csv('missing_codes.csv')
mvars = as.character(missing_codes$varname)
missing_codes$num_missing = as.integer(0)
for (row in 1:nrow(missing_codes)) {
  var = as.character(missing_codes[row, "varname"])
  code  = as.numeric(missing_codes[row, "missing_code"])
  nmissing = as.integer(sum(data[, var] == code))
  missing_codes$num_missing[missing_codes$varname==var] = nmissing
  data[, var] = na_if(data[, var], code)
}
# Convert all variables with <7 unique values to factor (and 3 additional variables)
factor_vars = c("isllb10", "birmon", "weekday")
for (var in colnames(data)) {
  if (length(unique(data[!is.na(data[, var]), var])) < 7 || var %in% factor_vars) {
    data[, var] = factor(data[, var])
  }
}
# label data
variable_labels_df = read.csv('variable_labels.csv')
variable_labels <- setNames(as.character(variable_labels_df$label), variable_labels_df$varname)
data <- Hmisc::upData(data, labels = variable_labels)

# Dataframe with missing dropped
df = data[complete.cases(data), ]
# Treatment reference level
df$tobacco <- relevel(df$tobacco, ref = "2")  # reference level: 2 = no for tobacco use during pregnancy
```



# Problem 1
**In Problem Set 1a, you used linear regression to relate infant health outcomes and maternal smoking during pregnancy.**


## Part (a)
**Under the assumption of random assignment conditional on the observables, what are the sources of misspecification bias in the estimates generated by the linear model estimated in Problem Set 1a?**

- functional form: what if the CEF isn't linear in parameters. Even if we have a linear CEF, what if we didn't include important interaction terms?



## Part (b)
**Now, consider a series estimator. Estimate the smoking effects using a flexible functional form for the control variables (e.g., higher order terms and interactions). What are the benefits and drawbacks to this approach?**

```{r 1B}
df1b = df %>% select(dbrwt, tobacco, csex, mrace3, preterm,
                     dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
                     disllb, dtotord, dmar, adequacy, nprevist)

# indicator vars (no higher order terms)
vars1 = names(Filter(is.factor, select(df1b, -dbrwt)))
# quantitative vars (need to create higher order terms)
vars2 = names(Filter(is.integer, select(df1b, -dbrwt)))

birthweight = df1b$dbrwt
x = df1b %>% select(-dbrwt)
# Create dummies from factor variables, all interactions, and squared continuous vars
formula1 = as.formula(paste("~ .^2 +", paste0("I(",vars2,"^2)", collapse=' + ') ))
xx <- model.matrix(formula1, x)[, -1]

# Series Regression
reg_1b = lm(birthweight ~ xx)
# rename the coefficients
names(reg_1b$coefficients) <- gsub("xx","",names(reg_1b$coefficients))
```

**Need to change to longtable before final submission**

```
A table enviroment cannot be broken across pages. Delete \begin{table}\centering and \end{table}, replace tabular with longtable, move \caption and label to immediately after \begin{longtable}{..}. And add \usepackage{longtable} to the preamble, of course.
```
```{r 1B tabl, results='asis'}
stargazer(reg_1b, title="Series Regression", header=FALSE, single.row=TRUE, 
          se = NULL, notes = 'SEs omitted for brevity',  type ="text", report = "vc*")  # , align = TRUE
```


## Part (c)
**Use the LASSO to determine which covariates (and higher order terms) to include in your regression from part (b). Do you end up dropping some covariates that you had thought might be necessary to include?**
```{r 1C}
# use glmnet with alpha=1 for lasso
reg_1c = glmnet(xx, birthweight, family="gaussian", alpha=1)
# print results (Df = # of variables, %Dev = R^2)
print(reg_1c)
# Limit the model to num_vars number of variables
num_vars = 20
# choose lowest lambda iteration that results in num_vars non-zero variables
i = max(which(abs(reg_1c$df - num_vars) == min(abs(reg_1c$df - num_vars))))
lambda = reg_1c$lambda[[i]]
print(paste0('# of variables in ', i, 'th iteration: ', sum(reg_1c$beta[, i] != 0)))
# print the i'th lasso regression coefficients
# print(reg_1c$beta[, i])

# Plot what the coefficients are doing as we increase lambda
plot(reg_1c, label=TRUE)
```
Each curve corresponds to a variable. It shows the path of its coefficient
  against the L1-norm of the whole coefficient vector as lambda varies. The top axis
  indicates the number of nonzero coefficients at the current lambda, which is the
  effective degrees of freedom (df) for the lasso.

Here are the non-zero coefficients:
```{r 1C table, results='asis'}
knitr::kable(reg_1c$beta[, i][reg_1c$beta[, i] != 0], 
             caption=paste0("Lasso Regression for top ", num_vars, " vars (lambda = ", lambda, ")"),
             col.names = 'Non-zero Coefficients', align = "l", digits = 3)
```

<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 2
**Describe the propensity score approach to the problem of estimating the average causal effect of smoking when the treatment is randomly assigned conditional on the observables. How does it reduce the dimensionality problem of multivariate matching? Try a few ways to estimate the effects of maternal smoking on birthweight:**

## Part (a)
**First create the propensity score. For our purposes let’s use a logit specification. First specify the logit using all of the “predetermined” covariates (don’t include interactions). Next, include only those “predetermined” covariates that enter significantly in the first logit specification. How comparable are the propensity scores? If they are similar does this imply that we have the “correct” set of covariates in the logit specification used for our propensity score?**
```{r 2a1}
# create the propensity score using logit
# using all of the “predetermined” covariates
df2a = df %>% select(tobacco, csex, mrace3, preterm,
                     dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
                     disllb, dtotord, dmar, adequacy, nprevist)
reg_2a1 <- glm(tobacco ~ ., data = df2a, family = "binomial")
```

```{r 2a1 table, results='asis'}
stargazer(reg_2a1, title="First logit regression", header=FALSE, single.row=TRUE, type ="text")
```

Removing sex of the child (`csex`) and prenatal adequacy (`adequacy`) from v

```{r 2a2}
# Try logit with only the significant covariates
df2a2 = df2a %>% select(-csex, -adequacy)
reg_2a2 <- glm(tobacco ~ ., data = df2a2, family = "binomial")
```

```{r 2a2 table, results='asis'}
stargazer(reg_2a2, title="Second logit regression", header=FALSE, single.row=TRUE, type ="text")
```


```{r 2a pscores, fig.show='hide'}
# Compare histograms of p-scores
p1 = reg_2a1 %>% predict(df2a, type = "response")
p2 = reg_2a2 %>% predict(df2a2, type = "response")
```

The histogram of the first predicted p-score is in orange, the second is in purple.
So this fully-pink histogram is meant to show that there is nearly complete
overlap in this histograms of the predicted p-scores.
```{r}
plot(hist(p1, plot=F), col=rgb(0,0,1,1/4), border=NA, xlim=c(0,1))  # first histogram
plot(hist(p2, plot=F), col=rgb(1,0,0,1/4), border=NA, xlim=c(0,1), add=T)  # second
```

We can see where the differences are that are more than 0.001 and the shape of the 
differences.
```{r}
threshold = 0.001
pdiff = p1-p2
plot(hist(p1[abs(pdiff) > threshold], plot=F), col=rgb(0,0,1,1/4), border=NA, xlim=c(0,1),
     main=paste('Histogram of p-scores that have differences greater than', threshold))
plot(hist(pdiff, plot=F), col=rgb(0,0,1,1/4), border=NA, main='Histogram of differences in p-scores')
```

Here's two density plots to reinforce the idea. The blue line is the density plot
for the first p-score and the red filling is the density plot for the second.
```{r}
plot(density(p1), col=rgb(0,0,1), xlim=c(0,1), lwd=3,
     main='Comparing p-score densities with and without significant variables')  # first kernal
polygon(density(p2), col="red", border=NA, xlim=c(0,1))  # second kernal filled in
```

Note that the maximum difference between any two predicted p-scores is `r max(abs(p1-p2))`.

## Part (b)
**Control directly for the estimated propensity scores using a regression analysis, and estimate an average treatment effect. State clearly the assumptions under which your estimate is correct.**


```{r}
# Control for p-score in regression analysis
df2b = df %>%
  select(dbrwt, tobacco, csex, mrace3, preterm,
         dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
         disllb, dtotord, dmar, adequacy, nprevist) %>%
  mutate(pscore = p1)
reg_2b = lm(dbrwt ~ ., data=df2b)

# Estimate ATE
ATE_2b = reg_2b$coefficients["tobacco1"]
```
The estimated ATE of tobacco use during pregnancy is `r round(ATE_2b,3)` -- smoking during pregnancy causes a `r abs(round(ATE_2b,3))` drop in grams of the birthweight of the child. This is correct if...


## Part (c)
**As discussed in class, one can use the estimated propensity scores to reweight the outcomes of non- smokers and estimate the average treatment effect. Compute an estimate of the average treatment effect and the “effect of the treatment on the treated” by appropriate reweighting of the data.**

```{r}
# Reweight data using p-score to weight


# Estimate ATE


# Estimate TOT with rewieghted data

```


## Part (d)
**Estimate the counterfactual densities relevant for the above part with a kernel density estimator. That is, estimate the density of birthweight (or log birthweight) if everyone smoked and again if no one smoked. Hint: Consider directly applying the Hirano, Imbens, and Ridder propensity score reweighting scheme in the context of estimating the densities of the treated and control groups (rather than the means of the treated and control groups). Stata has very useful preprogrammed commands. In addition to using the preprogrammed Stata command to compute/graph the kernel density over the entire range of birthweight, please also calculate by hand the kernel estimator at birthweight equals 3,000 grams (and provide the code you wrote that shows the calculation of the kernel estimator at this single point). Play around with a bandwidth starting with half the default Stata bandwidth. Choose the same bandwidth for all the pictures, and produce a (beautiful, production quality) figure depicting both densities.**

```{r}
# Estimate the counterfactual birthweight densities with a kernel density estimator
# See Joel's notes for kernel density estimator
# Play around with a bandwidth starting with half the default Stata bandwidth
# For stata bandwidht, see rkdensity.pdf page 9 in this ps1b github folder.
# You can also run on stata with no bandwidth specified, then print the
# default bandwidth used using `display r(bwidth)`
# Choose the same bandwidth for all the pictures


# Graph both kernel densities over range of birthweight in the same plot


# calculate the kernel estimator at birthweight equals 3,000 grams

```


## Part (e)
**Take one of your densities and display an estimate of the density using different bandwidths as well as the one you settled on. What happens with bigger (smaller) bandwidths?**

```{r}

```



## Part (f)
**What are the benefits of the weighting approach (from part c)? What are the potential drawbacks? Pay particular attention to to the issue of people with extremely high and extremely low values of the propensity score.**

```{r}

```



## Part (g)
**Present your findings and interpret the results on the relationship between birthweight and smoking. For the estimates in parts (b) and (c), consider which of the following conditions must hold in order for that estimate to be valid:**

- The treatment effect heterogeneity is linear in the propensity score.
- The treatment effect heterogeneity is not linear in the propensity score.
- The decision to smoke is completely randomly assigned.
- Conditional on the exogenous variables the decision to smoke is randomly assigned.


```{r}

```



<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 3
**A potentially more informative way to describe how birth weight affects smoking is to estimate the “non-parametric” conditional mean of birth weight as a function of the estimated probability of smoking, separately for smokers and non-smokers on the same graph. To do so, divide the data from smokers into 100 approximately equally spaced bins based on the estimated propensity score. Do the same for nonsmokers. Use the blocking estimator we discussed in class. Interpret your findings and relate them to the results in (2b).**

```{r}

```




<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 4
**Low birth weight births (less than 2500 grams) are considered particularly undesirable since they comprise a large share of infant deaths. Redo question 3 using an indicator for low birth weight birth as the outcome of interest. Interpret your findings.**




<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 5
**Let’s link matching back to regression. Consider the conditional expectation function $\mathbb{E}[birthweight\ |\ X]$, where $X$ contains the following variables: \texttt{rectype pldel3 cntocpop stresfip dmage mrace3 dmar adequacy csex dplural}.**

## Part (a)
**Develop a regression that you are confident estimates $\mathbb{E}[birthweight\ |\ X]$ as $N\to\infty$? Why are you confident that your regression gets the CEF right?**

```{r}
# Select variables
df5a = df %>% select(dbrwt, rectype, pldel3, cntocpop, stresfip, 
                     dmage, mrace3, dmar, adequacy, csex, dplural)

# Run regression

```

## Part (b)
**Now run the regression you propose above, but add the treatment (your binary smoking variable) as the righthand side variable of interest. Prove that if the treatment effect of smoking on birthweight is independent of the covariates in $X$, then exact matching and your regression estimate the same thing. You may assume the conditional independence assumption holds given the variables in $X$ listed above.**

```{r}
# Select vars and smoking indicator
df5b = df %>% select(dbrwt, rectype, pldel3, cntocpop, stresfip, 
                     dmage, mrace3, dmar, adequacy, csex, dplural)

# Run regression

```


## Part (c)
**Develop a weighted version of the exact matching estimator that estimates the same thing as the regression above (regardless of whether the treatment effect is independent of covariates).**


## Part (d)
**Estimate the weighted matching estimator you propose. Compare it to the regression estimate from part (b). Are they similar?**

```{r}

```


## Part (e)
**Is the sample size of your regression the same as the sample size of your matching estimator, or does the regression have more observations? If the regression has more observations, why don’t these extra observations influence the treatment effect estimate?**



## Part (f)
**Compute a standard error for your matching estimator using the formula from Imbens (2015). Specifically, note that your matching estimator should have a form
    \[\frac{1}{N_t}\sum_{d_i=1}w_iy_i - \frac{1}{N_c}\sum_{d_i=0}w_iy_i\]
    where $\sum_{d_i=1}w_i=N_t$ and $\sum_{d_i=0}w_i=N_c$. Then the conditional variance is approximately
    \[\sum_i\left(\frac{d_i}{N_t^2} + \frac{1-d_i}{N_c^2}w_i^2\hat\sigma_{d_i}^2(x_i) \right),\]
    where $\hat\sigma_{d_i}^2(x_i)=\frac{1}{2}(y_i - y_{nn(i)})$, and $y_{nn(i)}$ is the nearest neighbor to observation $i$ with the \textit{same} treatment status. Figure out the implicit weights $w_i$ in your estimator from part (d), and compute the conditional variance. Is it close to your regression coefficient variance?**
    
```{r}
# Compute a standard error for your matching estimator using the formula from Imbens (2015).


# compute the conditional variance of estimator from (d)


```




<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 6
**Concisely and coherently summarize your overall results, providing some intuition. Write it like you would the conclusion of a paper. In this summary, describe whether you think your best estimate of the effects of smoking is credibly identified. State why or why not.**















