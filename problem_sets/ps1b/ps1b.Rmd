---
title: "ARE 213 PS 1b"
author: "S. Sung, H. Husain, T. Woolley, A. Watt"
email: aaron@acwatt.net
date: "2021-10-18"
output:
  pdf_document:
    toc: true
    toc_depth: 2
header-includes:
   - \usepackage{dcolumn}
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'ARE_213_PS1b.pdf')) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps1b for ARE 213

Notes: Need to ps1b.R from os1b folder (without Rstudio being started first)
       if Rstudio is already started, the working directory will not be set
       to the ps1b/ folder
       
       \usepackage{dcolumn}: dcolumn is needed in latex compilation 
-->

```{r Settings, echo=FALSE}
# stargazer table type (html, latex, or text)
# Change to latex when outputting to PDF, html when outputting to html
table_type = "latex"
```




# Packages (omitted)
```{r packages, results='hide', message=FALSE, echo=F}
# install.packages("pacman")
# install.packages("tidyverse")
# install.packages("plm")
# install.packages('foreign')
# install.packages('stargazer')
# install.packages("finalfit")
# install.packages("glmnet")
# install.packages("jtools")
# install.packages("Hmisc")
# install.packages('zoo')
library(tidyverse)
library(foreign)
# library(xtable)
library(stargazer)
library(finalfit)
library(glmnet)
library(jtools)  # summ() for regression summaries
library(Hmisc)  # for 
```

# Data cleaning from PS 1a
```{r data_cleaning, results='hide'}
data = read.dta('ps1.dta')
missing_codes = read.csv('missing_codes.csv')
mvars = as.character(missing_codes$varname)
missing_codes$num_missing = as.integer(0)
for (row in 1:nrow(missing_codes)) {
  var = as.character(missing_codes[row, "varname"])
  code  = as.numeric(missing_codes[row, "missing_code"])
  nmissing = as.integer(sum(data[, var] == code))
  missing_codes$num_missing[missing_codes$varname==var] = nmissing
  data[, var] = na_if(data[, var], code)
}
# Convert all variables with <7 unique values to factor (and 3 additional variables)
factor_vars = c("isllb10", "birmon", "weekday")
for (var in colnames(data)) {
  if (length(unique(data[!is.na(data[, var]), var])) < 7 || var %in% factor_vars) {
    data[, var] = factor(data[, var])
  }
}
# label data
variable_labels_df = read.csv('variable_labels.csv')
variable_labels <- setNames(as.character(variable_labels_df$label), variable_labels_df$varname)
data <- Hmisc::upData(data, labels = variable_labels)

# Dataframe with missing dropped
df = data[complete.cases(data), ]
# Treatment reference level
df$tobacco <- relevel(df$tobacco, ref = "2")  # reference level: 2 = no for tobacco use during pregnancy
```



# Problem 1
**In Problem Set 1a, you used linear regression to relate infant health outcomes and maternal smoking during pregnancy.**


## Part (a)
**Under the assumption of random assignment conditional on the observables, what are the sources of misspecification bias in the estimates generated by the linear model estimated in Problem Set 1a?**

In problem set 1(a), we input observable characteristic in a linear fashion into our crude estimation with OLS. However, it is possible that CEF isn't linear in parameters, leading to bias. Also, even if we have a linear CEF, it's possible we didn't include important interaction terms.

In order to assume linearity, we need (1) joint normality with observables, indicator for maternal smoking, and our outcome variable of interest or (2) saturated model. In their absence, we could have used other non-linear estimation methods discussed in class.



## Part (b)

**Now, consider a series estimator. Estimate the smoking effects using a flexible functional form for the control variables (e.g., higher order terms and interactions). What are the benefits and drawbacks to this approach?**

```{r 1B}
df1b = df %>% select(dbrwt, fmaps, omaps, tobacco, 
                     csex, mrace3, preterm, dmage, dfage, dmeduc, dfeduc, 
                     ormoth, orfath, disllb, dtotord, dmar, adequacy, nprevist)

# indicator vars (no higher order terms)
vars1 = names(Filter(is.factor, select(df1b, -c(dbrwt, fmaps, omaps))))
# quantitative vars (need to create higher order terms)
vars2 = names(Filter(is.integer, select(df1b, -c(dbrwt, fmaps, omaps))))

birthweight = df1b$dbrwt
fiveminapgar = df1b$fmaps
oneminapgar = df1b$omaps

x = df1b %>% select(-c(dbrwt, fmaps, omaps))
# Create dummies from factor variables, all interactions, and squared continuous vars
formula1 = as.formula(paste("~ .^2 +", paste0("I(",vars2,"^2)", collapse=' + ') ))
xx <- model.matrix(formula1, x)[, -1]

# Series Regression
reg_1b_dbrwt = lm(birthweight ~ xx)
reg_1b_fmaps = lm(fiveminapgar ~ xx)
reg_1b_omaps = lm(oneminapgar ~ xx)

# rename the coefficients
names(reg_1b_dbrwt$coefficients) <- gsub("xx","",names(reg_1b_dbrwt$coefficients))
names(reg_1b_fmaps$coefficients) <- gsub("xx","",names(reg_1b_fmaps$coefficients))
names(reg_1b_omaps$coefficients) <- gsub("xx","",names(reg_1b_omaps$coefficients))
results_df = data.frame(type = 'Series Regression',
                        ate = reg_1b_dbrwt$coefficients['tobacco1'],
                        att = NA,
                        number_vars = ncol(xx)+1)
```

          
```{r 1B table abbreviated, results='asis', eval=T}
stargazer(reg_1b_dbrwt, reg_1b_fmaps, reg_1b_omaps,
          type = 'latex',
          keep = "tobacco1(?!:)",
          perl=TRUE, header = FALSE, table.placement = '!h',
          title = "Series Regressions", 
          align = TRUE, no.space = TRUE, font.size = "small",
          notes = c("For brevity, only the original variables are presented;",
                    "300+ second order / interaction terms are not shown."))
```


## Part (c)

**Use the LASSO to determine which covariates (and higher order terms) to include in your regression from part (b). Do you end up dropping some covariates that you had thought might be necessary to include?**

For brevity, I continue on with the exercise with only one dependent variable (dbrwt), skipping the others (fmaps and omaps). Running a simple lasso with all the included varibles from 1(b), and arbitrarily setting number of variables to be included as 20, we chose lambda = 14.23. This is the lowest lambda iteration that results in a model with 20 non-zero coefficients.

Under this setting, predicted impact of smoking on birthweight is -76.9.

We ended up dropping \texttt{dmage} and 'dfage', or age of parents, which we expected to have some correlation with infant's general health and/or weight. On the other hand, variables such as 'csex' and 'preterm', which we expected to have a stronger relationship with infant weight (i.e. male infant might be bigger on average, and previous baby's birth weight might be predictive of the baby of interest), are not dropped as expected.

One problem, however, with what we have done below is that while an interaction term that includes 'dmage' is included, the standalone variable 'dmage' is dropped, which might be undesirable. 


```{r 1C}
# use glmnet with alpha=1 for lasso
reg_1c = glmnet(xx, birthweight, family="gaussian", alpha=1)
# print results (Df = # of variables, %Dev = R^2)
print(reg_1c)
# Limit the model to num_vars number of variables
num_vars = 20
# choose lowest lambda iteration that results in num_vars non-zero variables
i = max(which(abs(reg_1c$df - num_vars) == min(abs(reg_1c$df - num_vars))))
lambda = reg_1c$lambda[[i]]

# Plot what the coefficients are doing as we increase lambda
op <- par(mfrow=c(1,2))
plot(reg_1c, "norm", label=TRUE)
plot(reg_1c, "lambda", label=TRUE)
par(op)
```
In both graphs, each curve corresponds to a variable. They show the path of its 
  coefficient against (in the left plot) the L1-norm of the whole coefficient 
  vector as lambda varies and (in the right plot) the values of log-lamba.
  The top axis indicates the number of nonzero coefficients at the current lambda, 
  which is the effective degrees of freedom (df) for the lasso. We see clearly 
  that as lambda increases more coefficients goes to zero, as intended in 
  model selection.

Here are the non-zero coefficients:
```{r 1C table, results='asis'}
knitr::kable(reg_1c$beta[, i][reg_1c$beta[, i] != 0], 
             caption=paste0("Lasso Regression for top ", num_vars, " vars (lambda = ", lambda, ")"),
             col.names = 'Non-zero Coefficients', align = "l", digits = 3)
```

<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 2

**Describe the propensity score approach to the problem of estimating the average causal effect of smoking when the treatment is randomly assigned conditional on the observables. How does it reduce the dimensionality problem of multivariate matching? Try a few ways to estimate the effects of maternal smoking on birthweight:**

## Part (a)

**First create the propensity score. For our purposes let’s use a logit specification. First specify the logit using all of the “predetermined” covariates (don’t include interactions). Next, include only those “predetermined” covariates that enter significantly in the first logit specification. How comparable are the propensity scores? If they are similar does this imply that we have the “correct” set of covariates in the logit specification used for our propensity score?**
```{r 2a1 regression}
# create the propensity score using logit
# using all of the “predetermined” covariates
df2a = df %>% select(tobacco, csex, mrace3, preterm,
                     dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
                     disllb, dtotord, dmar, adequacy, nprevist)
reg_2a1 <- glm(tobacco ~ ., data = df2a, family = "binomial")
# Try logit with only the significant covariates
df2a2 = df2a %>% select(-csex, -adequacy)
reg_2a2 <- glm(tobacco ~ ., data = df2a2, family = "binomial")
```

Model (1) below is with all the predetermined variables and model (2) is removing the insignificant variables.

```{r 2a1 table, results='asis'}
stargazer(reg_2a1, reg_2a2, title="Logit regressions", header=FALSE, single.row=TRUE, type=table_type)
```


```{r pscore histograms}
labs <- paste("Actual smoking status:", c("Yes", "No"))
p1_df <- data.frame(p1_score = predict(reg_2a1, type = "response"), tobacco = reg_2a1$model$tobacco) %>%
  mutate(tobacco = ifelse(tobacco == 1, labs[1], labs[2]))
p2_df <- data.frame(p2_score = predict(reg_2a2, type = "response"), tobacco = reg_2a2$model$tobacco) %>%
  mutate(tobacco = ifelse(tobacco == 1, labs[1], labs[2]))

ggplot(p1_df, aes(x=p1_score, fill = tobacco)) +
  geom_histogram(position = "identity", alpha = 0.5,
                 mapping = aes(y = stat(density))) +
  xlab("Probability of maternal smoking ") +
  ggtitle("P-scores estimated using all predetermined covariates")

ggplot(p1_df, aes(x=p1_score, fill = tobacco)) +
  geom_histogram(position = "identity", alpha = 0.5,
                 mapping = aes(y = stat(density))) +
  xlab("Probability of maternal smoking ") +
  ggtitle("P-scores estimated using significant predetermined covariates")
```

```{r 2a pscores}
# Compare histograms of p-scores
p1 = reg_2a1 %>% predict(df2a, type = "response")
p2 = reg_2a2 %>% predict(df2a2, type = "response")
```

The histogram of the first predicted p-score is in orange, the second is in purple.
So this fully-pink histogram is meant to show that there is nearly complete
overlap in this histograms of the predicted p-scores.
```{r 2a histogram overlap}
plot(hist(p1, plot=F), col=rgb(0,0,1,1/4), border=NA, xlim=c(0,1))  # first histogram
plot(hist(p2, plot=F), col=rgb(1,0,0,1/4), border=NA, xlim=c(0,1), add=T)  # second
```

We can see where the differences are that are more than 0.001 and the shape of the 
differences.
```{r 2a histogram differences}
threshold = 0.001
pdiff = p1-p2
plot(hist(p1[abs(pdiff) > threshold], plot=F), col=rgb(0,0,1,1/4), border=NA, xlim=c(0,1),
     main=paste('Histogram of p-scores that have differences greater than', threshold))
plot(hist(pdiff, plot=F), col=rgb(0,0,1,1/4), border=NA, main='Histogram of differences in p-scores')
```

Here's two density plots to reinforce the idea. The blue line is the density plot
for the first p-score and the red filling is the density plot for the second.
```{r 2c density plots}
plot(density(p1), col=rgb(0,0,1), xlim=c(0,1), lwd=3,
     main='Comparing p-score densities with and without significant variables')  # first kernel
polygon(density(p2), col="red", border=NA, xlim=c(0,1))  # second kernel filled in
```

Note that the maximum difference between any two predicted p-scores is `r max(abs(p1-p2))`.

Overall, the propensity scores are very similar. However, this does not imply that we have the "correct" set of covariates in the logit specification used for our propensity score. It merely shows that excluding the non-significant variables from the propensity score estimation won't affect the propensity score estimates. This doesn't suggest that we aren't missing any potentially important covariates. 

## Part (b)

**Control directly for the estimated propensity scores using a regression analysis, and estimate an average treatment effect. State clearly the assumptions under which your estimate is correct.**


```{r Problem 2b}
# Control for p-score in regression analysis
df2b = df %>%
  select(dbrwt, tobacco, csex, mrace3, preterm,
         dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
         disllb, dtotord, dmar, adequacy, nprevist) %>%
  mutate(pscore = p1)
reg_2b = lm(dbrwt ~ ., data=df2b)

# Estimate ATE
ATE_2b = reg_2b$coefficients["tobacco1"]
results_df = rbind(results_df, 
                   data.frame(type = 'Pscore Regression with covariates',
                              ate = ATE_2b,
                              att = NA,
                              number_vars = ncol(df2b)))

reg_2bb = lm(dbrwt ~ tobacco + pscore, data=df2b)
ATE_2bb = reg_2bb$coefficients["tobacco1"]
results_df = rbind(results_df, 
                   data.frame(type = 'Pscore Regression without covariates',
                              ate = ATE_2bb,
                              att = NA,
                              number_vars = 2+1))
```

The estimated ATE of tobacco use during pregnancy when including covariates and the pscore is `r round(ATE_2b,3)` -- smoking during pregnancy causes a `r abs(round(ATE_2b,3))` drop in grams of the birthweight of the child. When only including the pscore, the ATE is `r round(ATE_2bb,3)`. This is correct if we have unconfoundedness and we assume a constant treatment effect. 


## Part (c)

**As discussed in class, one can use the estimated propensity scores to reweight the outcomes of non-smokers and estimate the average treatment effect. Compute an estimate of the average treatment effect and the “effect of the treatment on the treated” by appropriate reweighting of the data.**

Imbens tells us that, after dividing by the sum of the weights in each treatment group, the feasible propensity-score-weighted ATE is:
\[
\hat\tau_{ATE} = \frac{\sum\limits_{i=1}^N\dfrac{Y_i\cdot D_i}
   {\hat p(X_i)}}{\sum\limits_{i=1}^N\dfrac{D_i}{\hat p(X_i)}} - 
   \frac{\sum\limits_{i=1}^N\dfrac{(1-D_i)\cdot Y_i}{1-\hat p(X_i)}}
   {\sum\limits_{i=1}^N\dfrac{1-D_i}{1-\hat p(X_i)}}
\]

```{r Problem 2c ATE}
# Reweight data using p-score to weight
df2c = df2b %>% 
  mutate(pscore = predict(reg_2a1, ., type = "response")) %>%
  mutate(weight = ifelse(tobacco == 1, 1/pscore, 1/(1-pscore)))
t_norm1 = sum((df2c$tobacco==1)*df2c$weight)  # smokers
c_norm1 = sum((df2c$tobacco==2)*df2c$weight)  # non-smokers
df2c = df2c %>% mutate(weight3 = ifelse(tobacco == 1, 1/pscore/t_norm1, 1/(1-pscore)/c_norm1))

# Estimate ATE
weight_mean_smoker_all = sum((df2c$tobacco==1) * df2c$weight * df2c$dbrwt) / t_norm1
weight_mean_nonsmoker_all = sum((df2c$tobacco==2) * df2c$weight * df2c$dbrwt) / c_norm1
ATE2c = weight_mean_smoker_all - weight_mean_nonsmoker_all
ATE2c
```
The propensity-score-weighted estimated average treatment effect is `r round(ATE2c,2)`.

Imbens (2004) in a review article states that the efficient TOT estimator of $\tau_{treated}$ is derived by weighting each observation by it's propensity score. This makes intuitive sense because we want to weight-up the observations that look like the treated observations and weight-down the observations that look like the untreated. After canceling out the propensity score in the first term and dividing by the sum of the weights for each treatment group, we get:
\[
\hat\tau_{ATT} = \frac{1}{N_T}\sum\limits_{i:D_i=1}Y_i - 
   \frac{\sum\limits_{i:D_i=0}Y_i\dfrac{\hat p(X_i)}{1-\hat p(X_i)}}
   {\sum\limits_{i:D_i=0}\dfrac{\hat p(X_i)}{1-\hat p(X_i)}}
= \frac{\sum\limits_{i=1}^ND_iY_i}{\sum\limits_{i=1}^ND_i} - 
   \frac{\sum\limits_{i=1}^N\dfrac{(1-D_i)\hat p(X_i)Y_i}{1-\hat p(X_i)}}
   {\sum\limits_{i=1}^N\dfrac{(1-D_i)\hat p(X_i)}{1-\hat p(X_i)}}
\]
```{r Problem 2c ATT}
# Reweight data for ATT
df2c = df2c %>% 
  mutate(weight2 = ifelse(tobacco == 1, 1, pscore/(1-pscore)))
t_norm2 = sum((df2c$tobacco==1)*df2c$weight2)  # smokers
c_norm2 = sum((df2c$tobacco==2)*df2c$weight2)  # non-smokers

# Estimate ATT
weight_mean_smoker_treat = sum((df2c$tobacco==1) * df2c$weight2 * df2c$dbrwt) / t_norm2
weight_mean_nonsmoker_treat = sum((df2c$tobacco==2) * df2c$weight2 * df2c$dbrwt) / c_norm2
ATT2c = weight_mean_smoker_treat - weight_mean_nonsmoker_treat
ATT2c
results_df = rbind(results_df, 
                   data.frame(type = 'Pscore weighted data',
                              ate = ATE2c,
                              att = ATT2c,
                              number_vars = NA))
```
The propensity-score-weighted estimated average treatment effect on the treated is `r round(ATT2c,2)`. If we take this result as significantly different from the ATE, then the effect of smoking on birthweight is stronger for those who look like smokers than it is in the general population. This seems to suggest that there are other, possibly unobserved, characteristics that decrease birthweight that are more common in the population that look like non-smokers than in population that looks like smokers.

## Part (d)

**Estimate the counterfactual densities relevant for the above part with a kernel density estimator. That is, estimate the density of birthweight (or log birthweight) if everyone smoked and again if no one smoked. Hint: Consider directly applying the Hirano, Imbens, and Ridder propensity score reweighting scheme in the context of estimating the densities of the treated and control groups (rather than the means of the treated and control groups). Stata has very useful preprogrammed commands. In addition to using the preprogrammed Stata command to compute/graph the kernel density over the entire range of birthweight, please also calculate by hand the kernel estimator at birthweight equals 3,000 grams (and provide the code you wrote that shows the calculation of the kernel estimator at this single point). Play around with a bandwidth starting with half the default Stata bandwidth. Choose the same bandwidth for all the pictures, and produce a (beautiful, production quality) figure depicting both densities.**

Morgan and Todd (2008) gives us the weights for ATT and ATC (average treatment effect on the controls). These come from weighting observations by $1-p(X_i)$ instead of $p(X_i)$ to weight toward the control group:

\[
\begin{aligned}
\text{For }D_i=1: &\quad w_{i,ATT} = 1\\
\text{For }D_i=0: &\quad w_{i,ATT} = \frac{\hat p(X_i)}{1-\hat p(X_i)}\\
\text{and} \qquad&\\
\text{For }D_i=1: &\quad w_{i,ATC} = \frac{1-\hat p(X_i)}{\hat p(X_i)}\\
\text{For }D_i=0: &\quad w_{i,ATC} = 1 \\
\end{aligned}
\]

We can use weighted kernel density estimation with the weights above:

```{r Problem 2d}
# Stata default bandwidth
m = min(var(df2c$dbrwt), IQR(df2c$dbrwt)/1.349)
h_stata = 0.9*m/nrow(df2c)^(1/5)  # 44.14637 grams
df2d = df2c

# Create a vector of birthwieght values for plotting the density
x_vec = seq(min(df2d$dbrwt), max(df2d$dbrwt), length.out=1000)

# Define the kernel
kernel_fun <- function(x, h){ # Epanechnikov Kernel with bandwidth h
  weight = (3/4)*(1-(x/h)^2)*as.numeric(abs(x/h)<1)
  return(weight)
}

# Define the kernel density
density_fun <- function(x, df, h) { # Kernel density estimate with data X and weights w
  withCallingHandlers({
    # h = h_stata*2
    n = nrow(df)
    df = df %>% filter(between(dbrwt, x-h, x+h))
    if (nrow(df) == 0) {return(0)}
    kernel_vec = sapply(x - df$dbrwt, kernel_fun, h=h)
    f = sum(df$weight * kernel_vec) / (n*h) #
    return(f)
  }, warning=function(w) {
      if (startsWith(conditionMessage(w), "between() called on numeric"))
          invokeRestart("muffleWarning")
  })
}

# Calculate starting bandwidths (based on Stata formula)
m1 = min(var(filter(df2d, tobacco==1)$dbrwt), IQR(filter(df2d, tobacco==1)$dbrwt)/1.349)
h1 = 0.9*m/nrow(filter(df2d, tobacco==1))^(1/5)
m2 = min(var(filter(df2d, tobacco==2)$dbrwt), IQR(filter(df2d, tobacco==2)$dbrwt)/1.349)
h2 = 0.9*m/nrow(filter(df2d, tobacco==2))^(1/5)
# estimate counterfactual densities (takes about 16 seconds for both over 1000 point x_vec)
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=2*h1)
dens2_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==2), h=2*h2)

# Caclulate area under the curve to use as normalization constants for the PDFs
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
norm_cons2 = sum(diff(x_vec) * zoo::rollmean(dens2_custom, 2))

# Use normalization constants to scale up to true PDFs
dens_df = rbind(data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes"),
                data.frame(birthweight=x_vec, f=(dens2_custom / norm_cons2), smoker="no"))

# Plot
mean_lines = data.frame(avg = c(weight_mean_smoker_all, weight_mean_nonsmoker_all),
    name = c(paste("weighted mean smoker-babies weight =", round(weight_mean_smoker_all, 1)),
             paste("weighted mean nonsmoker-babies weight =", round(weight_mean_nonsmoker_all, 1))))
p <- ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5) +
  geom_vline(data=mean_lines, mapping=aes(xintercept=avg), color="blue") +
  geom_text(data=mean_lines, mapping=aes(x=avg, y=0, label=name), size=4, angle=90, vjust=c(-0.4, 1.2), hjust=0)
p
```

Calculate the kernel estimator at birthweight equals 3,000 grams:
```{r Problem 2d cont.}
# for smokers
density_fun(3000, filter(df2d, tobacco==1), 2*h1) / norm_cons1
# for nonsmokers
density_fun(3000, filter(df2d, tobacco==2), 2*h2) / norm_cons1
```


## Part (e)
**Take one of your densities and display an estimate of the density using different bandwidths as well as the one you settled on. What happens with bigger (smaller) bandwidths?**

Smokers' density with bandwidth = 1/2 * Stata bandwidth

```{r Problem 2e 0.5, echo=F}
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=0.5*h1)
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
dens_df = data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes")
ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5)
```

Smokers' density with bandwidth = 1 Stata bandwidth

```{r Problem 2e 1, echo=F}
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=h1)
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
dens_df = data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes")
ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5)
```

Smokers' density with bandwidth = 2 Stata bandwidth

```{r Problem 2e 2, echo=F}
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=2*h1)
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
dens_df = data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes")
ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5)
```

As we increase the bandwidth, the density esimtate gets smoother and smoother.


## Part (f)
**What are the benefits of the weighting approach (from part c)? What are the potential drawbacks? Pay particular attention to to the issue of people with extremely high and extremely low values of the propensity score.**

Weighting on the propensity score, we can reduce the dimensionality of the covariate space to compare observations that have similar probabilities of selection into the treatment group. If we have good covariate overlap, this means we can estimate treatment effects based on comparisons of control and treatment observations that "look alike." 

If we don't have good covariate overlap, however, we may have very few observations that are matched to the opposite treatment group. We expect the majority of the control observations to have a low propensity score (since it's estimating the probability of treatment based on covariates) and the majority of treatment observations to have high propensity scores. Without good covariate overlap, we would expect very few treatment observations to have very low p-scores (near 0) and very few control observations to have high p-scores (near 1).

Since our weights are $1/p(X)$ for treated observations, the weights for the rare treatment observations with p-scores near 0 can be extremely large and thus we might be relying on relatively few observations to estimate the treatment effect. Similarly, the weights are $1/(1-p(X))$ for control observations and we could be heavily weighting few observations with p-scores near 1.  If we trust the outlier untreated observations, this may be reasonable, however the fact that they are outliers among the untreated is also a worrying sign such outliers may be unrepresentative as counterfactual controls.


## Part (g)

**Present your findings and interpret the results on the relationship between birthweight and smoking. For the estimates in parts (b) and (c), consider which of the following conditions must hold in order for that estimate to be valid:**

(i) The treatment effect heterogeneity is linear in the propensity score.
(ii) The treatment effect heterogeneity is not linear in the propensity score.
(iii) The decision to smoke is completely randomly assigned.
(iv) Conditional on the exogenous variables the decision to smoke is randomly assigned.

For part (b) where we estimate the ATE using a regression on the treatment and the pscore, we estimate an effect of `r round(ATE_2bb,1)` -- that is, on average, we expect that smoking during pregnancy decreases the birthweight of the child by `r round(abs(ATE_2bb),1)` grams for the general population (assuming our sample is representative of the general population). This requires that condition (iv) holds, and additionally, that the treatment effect is homogeneous since we are regressing on just the treatment indicator and the pscore.

For part (c), we use the pscore to weight the observations to create balanced set of covariates and estimate the average treatment effect to be `r round(ATE2c,1)` -- on average, we expect that smoking during pregnancy decreases the birthweight of the child by `r round(abs(ATE2c),1)` grams for the general population. We also estimate the average effect on the treated to be `r round(ATT2c,1)` -- on average for women that smoke, we expect that smoking during pregnancy decreases the birthweight of the child by `r round(abs(ATE2c),1)` grams. These results both require conditions (i) and (iv) to be valid.

We can assess the homogeneous treatment effect assumption by comparing the shape of the counterfactual distributions. Below, we have shifted the smokers' distribution up by the ATE estimated in part (c). We can see that the distributions are extremely close, giving evidence that the treatment effect is fairly homogenous in across the pscore values.

```{r Problem 2g}
dens_df2 = dens_df %>%
  mutate(birthweight = ifelse(smoker=='yes', birthweight - ATE2c, birthweight))

ggplot(dens_df2, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker), alpha = 0.4, color='black', size=0.3)
```

It is unclear to us if the pscore data weighting method relies on the assumption of heterogeneous treamtent effects, if this type of counterfactual density comparison is circular reasoning. 


<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 3
**A potentially more informative way to describe how birth weight affects smoking is to estimate the “non-parametric” conditional mean of birth weight as a function of the estimated probability of smoking, separately for smokers and non-smokers on the same graph. To do so, divide the data from smokers into 100 approximately equally spaced bins based on the estimated propensity score. Do the same for nonsmokers. Use the blocking estimator we discussed in class. Interpret your findings and relate them to the results in (2b).**

```{r Problem 3}
#Prepping dataset arranged by pscore
df3 <- df %>% 
  select(dbrwt, tobacco) %>%
  mutate(pscore = p1, smoke = ifelse(tobacco == 1, 1,0)) %>%
  select(-tobacco) %>% 
  arrange(pscore)

# Check Overlap Assumption
minmax <- df3 %>% 
            group_by(smoke) %>%
            summarise(max = max(pscore), min = min(pscore))

#Creating bins separately for smokers and non-smokers
df3_smoke <- df3 %>%
  filter(smoke == 1) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

df3_nosmoke <- df3 %>%
  filter(smoke == 0) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

#Generating t_k (treatment effects for bins)
df3_smoke_mean <- df3_smoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_smoke = mean(dbrwt), N_smoke = n())

df3_nosmoke_mean <- df3_nosmoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_nosmoke = mean(dbrwt), N_nosmoke = n())

#Generating t (weighted total treatment effect)
df3_merged <- full_join(df3_nosmoke_mean, df3_smoke_mean, by = "pscore_bin") %>% 
  filter(is.na(bin_mean_nosmoke) == 0 & is.na(bin_mean_smoke) == 0) %>%
  mutate(dbrwt_diff = bin_mean_smoke - bin_mean_nosmoke, N = N_smoke + N_nosmoke)

N_total_3 <- sum(df3_merged$N)

df3_merged <- df3_merged %>% 
  mutate(t_k_weighted = dbrwt_diff * (N/N_total_3))  

t_3 <- sum(df3_merged$t_k_weighted)
results_df = rbind(results_df, 
                   data.frame(type = 'Blocking on pscore (continuous weight)',
                              ate = t_3,
                              att = NA,
                              number_vars = NA))
```

First, for each group of smokers and nonsmokers, we see that the range of estimated propensity score is (`r round(as.numeric(minmax[1,3]), 3)`, `r round(as.numeric(minmax[1,2]), 4)`) for nonsmokers and (`r round(as.numeric(minmax[2,3]), 4)`, `r round(as.numeric(minmax[2,2]), 4)`) for smokers. The range is very similar, meaning overlap assumption is satisfied, and trimming for blocking is not needed. This also ensures that what we have done above, which is creating 100 equal bins between the minimum and maximum propensity score for each group, will be very similar to creating the same 100 bins for the two groups such that first bin is [0, 0.01] and so on.

Using the blocking estimator, we find a smoking treatment effect of `r round(t_3,3)`


<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 4
**Low birth weight births (less than 2500 grams) are considered particularly undesirable since they comprise a large share of infant deaths. Redo question 3 using an indicator for low birth weight birth as the outcome of interest. Interpret your findings.**

```{r Problem 4}
# Prepping dataset with indicator variable 
df4 <- df3 %>% 
  mutate(low_dbrwt = ifelse(dbrwt < 2500, 1, 0))

# Creating bins separately for smokers and non-smokers
df4_smoke <- df4 %>%
  filter(smoke == 1) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

df4_nosmoke <- df4 %>%
  filter(smoke == 0) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

# Generating t_k (treatment effects for bins)
df4_smoke_mean <- df4_smoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_smoke = mean(low_dbrwt), N_smoke = n())

df4_nosmoke_mean <- df4_nosmoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_nosmoke = mean(low_dbrwt), N_nosmoke = n())

# Generating t (weighted total treatment effect)
df4_merged <- full_join(df4_nosmoke_mean, df4_smoke_mean, by = "pscore_bin") %>% 
  filter(is.na(bin_mean_nosmoke) == 0 & is.na(bin_mean_smoke) == 0) %>%
  mutate(low_dbrwt_diff = bin_mean_smoke - bin_mean_nosmoke, N = N_smoke + N_nosmoke)

N_total_4 <- sum(df4_merged$N)

df4_merged <- df4_merged %>% 
  mutate(t_k_weighted = low_dbrwt_diff * (N/N_total_4))  

t_4 <- sum(df4_merged$t_k_weighted)
results_df = rbind(results_df, 
                   data.frame(type = 'Blocking on pscore (indicator for low weight)',
                              ate = t_4,
                              att = NA,
                              number_vars = NA))
```

Using the blocking estimator, we find a smoking treatment effect of `r round(t_4,3)`. That is smokers are `r round(t_4,3)*100` percentage points more likely to have a baby with low birth weight than non-smokers. 

<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 5
**Let’s link matching back to regression. Consider the conditional expectation function $\mathbb{E}[birthweight\ |\ X]$, where $X$ contains the following variables: \texttt{rectype pldel3 cntocpop stresfip dmage mrace3 dmar adequacy csex dplural}.**

## Part (a)
**Develop a regression that you are confident estimates $\mathbb{E}[birthweight\ |\ X]$ as $N\to\infty$? Why are you confident that your regression gets the CEF right?**


Saturated model for discrete regressors is a sufficient condition for a linear CEF. Hence, I would create a dummy variable for each unique combination of covariate values and run linear regression on these newly created dummy variables to estimate CEF.



## Part (b)
**Now run the regression you propose above, but add the treatment (your binary smoking variable) as the righthand side variable of interest. Prove that if the treatment effect of smoking on birthweight is independent of the covariates in $X$, then exact matching and your regression estimate the same thing. You may assume the conditional independence assumption holds given the variables in $X$ listed above.**


```{r Problem 5b, eval=F}
# Select variables
df5a = df %>% 
          select(dbrwt, tobacco, rectype, pldel3, cntocpop, stresfip, 
                 dmage, mrace3, dmar, adequacy, csex, dplural) %>% 
          mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Create factor variable (for dummies needed for regression) for unique combination of covariate values
df5a_unique = df5a %>% 
                select(-c(dbrwt, tobacco)) %>% 
                distinct() %>% 
                arrange(rectype, pldel3, cntocpop, stresfip, 
                  dmage, mrace3, dmar, adequacy, csex, dplural) %>%
                mutate(uniq = row_number())
df5a <- full_join(df5a, df5a_unique, by = c("rectype", "pldel3", "cntocpop", 
                  "stresfip", "dmage", "mrace3", "dmar", "adequacy", "csex", "dplural"))
df5a <- df5a %>% mutate(uniq = as.factor(uniq))

# Run regression on dbrwt with saturated model (incl tobacco)
start_t = proc.time()
reg_5b <- lm(dbrwt ~ tobacco + uniq, data = df5a)
end_t = proc.time()
print('time to regress:')
print(end_t[3] - start_t[3])
ATE5b = reg_5b$coefficients['tobacco1']

results_df = rbind(results_df, 
                   data.frame(type = 'Saturated regression',
                              ate = ATE5b,
                              att = NA,
                              number_vars = length(levels(df5b$uniq)) + 2))
```

```{r Problem 5b table, results='asis', eval=F}
stargazer(reg_5b,
          type = 'latex',
          keep = "tobacco1",
          perl=TRUE, header = FALSE, table.placement = '!h',
          title = "Saturated Regression", 
          align = TRUE, no.space = TRUE, font.size = "small",
          notes = c("For brevity, only the treatment is presented;",
                    paste(length(levels(df5a$uniq)), "indicator for cells are not shown.")))
```

Let Y = dbrwt, D = tobacco, and X = covariates listed for quesiton 5a. Assume $Y_i = \alpha + \beta D_i + \delta h(x_i) + v_i$ and thereby $Y_i = \alpha + \beta \tilde{D_i} + \delta h(x_i) + u_i$, where $\tilde{D_i} = D_i - \mathbb{E}[D_i|X_i]$ and $u_i = \epsilon_i + \beta \mathbb{E}[D_i|X_i]$. We showed, in class, that $\mathbb{E}[u_i \tilde{D_i}] = 0$ under un-confounded-ness and homogeneous treatment, and can get a consistent estimate of $\beta$ if the CEF is known. While our CEF is not known, given we have linear CEF with a saturated model, we can simply include X's as additional control.

Let matrix $Z = [D \; E[D|X_i]]$.

\[
\begin{aligned}
\tau_{(OLS \; 5b)} &= \frac{\mathbb{E}[Z_i Y_i]}{Var[Z_i]} &  \\
                   &= \frac{\mathbb{E}[\tilde{D_i}Y_i]}{Var[\tilde{D_i}]} & \text{given consistent estimate} \\
                   &= \frac{\mathbb{E}[\mathbb{E}[\tilde{D}Y_i|D_i, X_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}\mathbb{E}[Y_i|D_i, X_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}(\mathbb{E}[Y_i|D_i=0, X_i] + \mathbb{E}[\tau_i|D_i, X_i]D_i)]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}(\mathbb{E}[Y_i|D_i=0, X_i] + \mathbb{E}[\tau_i|X_i]D_i)]}{Var[\tilde{D_i}]} & \text{CIA} \\
                   &= \frac{\mathbb{E}[\tilde{D}\mathbb{E}[Y_i|D_i=0, X_i] + \tilde{D}D_i\mathbb{E}[\tau_i]]}{Var[\tilde{D_i}]} & \text{Trt independent of X} \\
                   &= \frac{\mathbb{E}[D_i\mathbb{E}[Y_i|D_i=0, X_i] -  \mathbb{E}[D_i|X_i]\mathbb{E}[Y_i|D_i=0, X_i] + \tilde{D}D_i\mathbb{E}[\tau_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[D_i\mathbb{E}[Y_i|D_i=0, X_i]] -  \mathbb{E}[\mathbb{E}[D_i|X_i]\mathbb{E}[Y_i|D_i=0, X_i]] + \mathbb{E}[\tilde{D}D_i\mathbb{E}[\tau_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[D_i]\mathbb{E}[Y_i|D_i=0, X_i] -  \mathbb{E}[D_i]\mathbb{E}[Y_i|D_i=0, X_i] + \mathbb{E}[\tilde{D}D_i]\mathbb{E}[\tau_i]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}D_i]\mathbb{E}[\tau_i]}{Var[\tilde{D_i}]} \\
                   &= \frac{[\mathbb{E}[D_i^2] - \mathbb{E}[D_i]^2]\mathbb{E}[\tau_i]}{Var[\tilde{D_i}]} \\
                   &= \frac{Var[D_i]\mathbb{E}[\tau_i]}{Var[D_i]} \\
                   &= \mathbb{E}[\tau_i]
\end{aligned}
\]

Hence, we know our regression from 5b estimates Average Treatment Effect.

Furthermore, we have assumed CIA holds, that conditioning on X's, materal tobacco is as good as randomly assigned. Hence, matching exactly on X's and comparing outcomes within the match across treatment and control groups should also derive Average Treatment Effect. Hence, our regression from 5b would estimate the same thing as exact matching.


## Part (c)
**Develop a weighted version of the exact matching estimator that estimates the same thing as the regression above (regardless of whether the treatment effect is independent of covariates).**

\[ 
\begin{aligned}
  \hat{\tau}_M &= \frac{1}{N_T} \sum_{i : D_i=1} \left[y_i - \sum_{j : D_j=0} w_i(j) y_j\right]\\[2em]
  \text{where } w_i(j) &= 
  \begin{cases}
    \frac{1}{N_{CC}(i)} & \text{if } i \& j\text{ share a cell}\\[.5em]
    0 & \text{o.w.}\\
  \end{cases}
\end{aligned} 
\]

Given discrete covariates establishing cells for exact matching, we can set $w_i(j)$ equal to 0 for control observations $j$ that do not get (exactly) matched to treatment observation $i$; for control observations with same covariates matched to $i$, we would set $w_i(j)$ equal to $\frac{1}{N_{CC}(i)}$, where $N_{CC}(i)$ is number of control observations that share the same covariate cell as observation $i$. 



## Part (d)
**Estimate the weighted matching estimator you propose. Compare it to the regression estimate from part (b). Are they similar?**

```{r Problem 5d, eval=F}
# Make two df5a--one of treated and one of untreated. 
df5c <- df5a %>%
  select(dbrwt, tobacco, uniq)
df5c_t <- subset(df5c, tobacco ==1)
df5c_t <- df5c_t %>%
  mutate(dbrwt_t = dbrwt) %>%
  select(dbrwt_t, uniq)

df5c_c <- subset(df5c, tobacco ==2)
df5c_c <- df5c_c %>%
  mutate(dbrwt_c = dbrwt) %>%
  select(dbrwt_c, uniq)

# Get N treated and controls in each bucket
df5c_t2 <- df5c_t %>%
  group_by(uniq) %>%
  mutate(N_t = n())

df5c_c2 <- df5c_c %>%
  group_by(uniq) %>%
  mutate(count = n())

# Collapse controls down to uniq and take average of weight
df5c_c3 <- aggregate(df5c_c2$count, list(df5c_c2$uniq), mean)
df5c_c3 <- df5c_c3 %>%
  rename(uniq = Group.1, N_c = x)

df5c_c4 <- aggregate(df5c_c$dbrwt_c, list(df5c_c$uniq), mean)
df5c_c4 <- df5c_c4 %>%
  rename(uniq = Group.1, dbrwt_c = x)

df5c_c5 <- merge(df5c_c3, df5c_c4, by = "uniq", all.x = F, all.y = F)


# Merge the treated and collapsed control obs
df5c2 <- merge(df5c_t2, df5c_c5, by = "uniq", all.x = F, all.y = F)
df5c2 <- df5c2 %>%
  mutate(diff_wt = dbrwt_t - dbrwt_c)

summary(df5c2$diff_wt)[4]
```

Comparing the exact matching estimate of -215.5 to saturated regression estimate of -217.6, we see that our estimates are very similar.


## Part (e)
**Is the sample size of your regression the same as the sample size of your matching estimator, or does the regression have more observations? If the regression has more observations, why don’t these extra observations influence the treatment effect estimate?**

The regression has more observations because it includes all the observation in the data while the matching estimator includes only those treated and untreated observations with a match in the untreated and treated group, respectively. However, this discrepancy do not influence the treatment effect estimate because treated and untreated observations without any untreated or treated match (respectively) are given a weight of zero.


## Part (f)
**Compute a standard error for your matching estimator using the formula from Imbens (2015). Specifically, note that your matching estimator should have a form
    \[\frac{1}{N_t}\sum_{d_i=1}w_iy_i - \frac{1}{N_c}\sum_{d_i=0}w_iy_i\]
    where $\sum_{d_i=1}w_i=N_t$ and $\sum_{d_i=0}w_i=N_c$. Then the conditional variance is approximately
    \[\sum_i\left(\frac{d_i}{N_t^2} + \frac{1-d_i}{N_c^2}w_i^2\hat\sigma_{d_i}^2(x_i) \right),\]
    where $\hat\sigma_{d_i}^2(x_i)=\frac{1}{2}(y_i - y_{nn(i)})$, and $y_{nn(i)}$ is the nearest neighbor to observation $i$ with the \textit{same} treatment status. Figure out the implicit weights $w_i$ in your estimator from part (d), and compute the conditional variance. Is it close to your regression coefficient variance?**
    
```{r Problem 5f}
# Compute a standard error for your matching estimator using the formula from Imbens (2015).


# compute the conditional variance of estimator from (d)


```




<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 6
**Concisely and coherently summarize your overall results, providing some intuition. Write it like you would the conclusion of a paper. In this summary, describe whether you think your best estimate of the effects of smoking is credibly identified. State why or why not.**

In conclusion, the treatment effect of mother's smoking habits on baby birthweight appears to be robust across a number of identification strategies. Each method--be it matching by propensity score or on covariates, or using a egression or direct matching estimator--produces extremely similar results. While surprising on the surface, the reason for this is simple--they all use essentially the same mechanics under the hood. 

Each of these methods compares treated and untreated observations based on similar characteristics. In the case of propensity score matching, observations are weighted by their likelihood of being treated, giving more weight to untreated observations that by all other measures look like treated observations. In the case of exact matching, treated observations are paired with untreated observations based on relevant characteristics and their differences are computed. For these reasons, we believe the true effect of smoking on baby birthweight to be around -220.

```{r echo=F, eval=F}
# qwraps2

our_summary1 <-
  list("Miles Per Gallon" =
       list("Series Regression"       = ~ reg_1b_dbrwt$coefficients['tobacco1'],
            "Regression with pscore"       = ~ reg_2bb$coefficients["tobacco1"],
       "Displacement" =
       list("min"       = ~ min(disp),
            "median"    = ~ median(disp),
            "max"       = ~ max(disp),
            "mean (sd)" = ~ qwraps2::mean_sd(disp)),
       "Weight (1000 lbs)" =
       list("min"       = ~ min(wt),
            "max"       = ~ max(wt),
            "mean (sd)" = ~ qwraps2::mean_sd(wt)),
       "Forward Gears" =
       list("Three" = ~ qwraps2::n_perc0(gear == 3),
            "Four"  = ~ qwraps2::n_perc0(gear == 4),
            "Five"  = ~ qwraps2::n_perc0(gear == 5))
       )
```













