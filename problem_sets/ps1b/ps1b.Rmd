---
title: "ARE 213 PS 1b"
author: "S. Sung, H. Husain, T. Woolley, A. Watt"
email: aaron@acwatt.net
date: "2021-10-18"
output:
  html_document:
    toc: true
    toc_depth: 2
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'ARE_213_PS1b.html')) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps1b for ARE 213

Notes: Need to ps1b.R from os1b folder (without Rstudio being started first)
       if Rstudio is already started, the working directory will not be set
       to the ps1b/ folder
-->

```{r Settings, echo=FALSE}
# stargazer table type (html, latex, or text)
# Change to latex when outputting to PDF, html when outputting to html
table_type = "html"
```




# Packages
```{r packages, results='hide', message=FALSE}
# install.packages("pacman")
# install.packages("tidyverse")
# install.packages("plm")
# install.packages('foreign')
# install.packages('stargazer')
# install.packages("finalfit")
# install.packages("glmnet")
# install.packages("jtools")
# install.packages("Hmisc")
# install.packages('zoo')
library(tidyverse)
library(foreign)
# library(xtable)
library(stargazer)
library(finalfit)
library(glmnet)
library(jtools)  # summ() for regression summaries
library(Hmisc)
```

# Data cleaning from PS 1a
```{r data_cleaning}
data = read.dta('ps1.dta')
missing_codes = read.csv('missing_codes.csv')
mvars = as.character(missing_codes$varname)
missing_codes$num_missing = as.integer(0)
for (row in 1:nrow(missing_codes)) {
  var = as.character(missing_codes[row, "varname"])
  code  = as.numeric(missing_codes[row, "missing_code"])
  nmissing = as.integer(sum(data[, var] == code))
  missing_codes$num_missing[missing_codes$varname==var] = nmissing
  data[, var] = na_if(data[, var], code)
}
# Convert all variables with <7 unique values to factor (and 3 additional variables)
factor_vars = c("isllb10", "birmon", "weekday")
for (var in colnames(data)) {
  if (length(unique(data[!is.na(data[, var]), var])) < 7 || var %in% factor_vars) {
    data[, var] = factor(data[, var])
  }
}
# label data
variable_labels_df = read.csv('variable_labels.csv')
variable_labels <- setNames(as.character(variable_labels_df$label), variable_labels_df$varname)
data <- Hmisc::upData(data, labels = variable_labels)

# Dataframe with missing dropped
df = data[complete.cases(data), ]
# Treatment reference level
df$tobacco <- relevel(df$tobacco, ref = "2")  # reference level: 2 = no for tobacco use during pregnancy
```



# Problem 1
**In Problem Set 1a, you used linear regression to relate infant health outcomes and maternal smoking during pregnancy.**


## Part (a)
**Under the assumption of random assignment conditional on the observables, what are the sources of misspecification bias in the estimates generated by the linear model estimated in Problem Set 1a?**

In problem set 1(a), we input observable characteristic in a linear fashion into our crude estimation with OLS. However, it is possible that CEF isn't linear in parameters, leading to bias. Also, even if we have a linear CEF, it's possible we didn't include important interaction terms.

In order to assume linearity, we need (1) joint normality with observables, indicator for maternal smoking, and our outcome variable of interest or (2) saturated model. In their absence, we could have used other non-linear estimation methods discussed in class.



## Part (b)

**Now, consider a series estimator. Estimate the smoking effects using a flexible functional form for the control variables (e.g., higher order terms and interactions). What are the benefits and drawbacks to this approach?**

```{r 1B}
df1b = df %>% select(dbrwt, fmaps, omaps, tobacco, 
                     csex, mrace3, preterm, dmage, dfage, dmeduc, dfeduc, 
                     ormoth, orfath, disllb, dtotord, dmar, adequacy, nprevist)

# indicator vars (no higher order terms)
vars1 = names(Filter(is.factor, select(df1b, -c(dbrwt, fmaps, omaps))))
# quantitative vars (need to create higher order terms)
vars2 = names(Filter(is.integer, select(df1b, -c(dbrwt, fmaps, omaps))))

birthweight = df1b$dbrwt
fiveminapgar = df1b$fmaps
oneminapgar = df1b$omaps

x = df1b %>% select(-c(dbrwt, fmaps, omaps))
# Create dummies from factor variables, all interactions, and squared continuous vars
formula1 = as.formula(paste("~ .^2 +", paste0("I(",vars2,"^2)", collapse=' + ') ))
xx <- model.matrix(formula1, x)[, -1]

# Series Regression
reg_1b_dbrwt = lm(birthweight ~ xx)
reg_1b_fmaps = lm(fiveminapgar ~ xx)
reg_1b_omaps = lm(oneminapgar ~ xx)

# rename the coefficients
names(reg_1b_dbrwt$coefficients) <- gsub("xx","",names(reg_1b_dbrwt$coefficients))
names(reg_1b_fmaps$coefficients) <- gsub("xx","",names(reg_1b_fmaps$coefficients))
names(reg_1b_omaps$coefficients) <- gsub("xx","",names(reg_1b_omaps$coefficients))
```

```{r 1B table abbreviated, results='asis'}
stargazer(reg_1b_dbrwt, reg_1b_fmaps, reg_1b_omaps,
          type = 'latex',
          omit=":",
          title = "Series Regressions", 
          align = TRUE, no.space = TRUE, font.size = "small",
          notes = "For brevity, only the original variables are presented;
                   300+ second order / interaction terms are not shown.")
```


## REVIEW THIS BEFORE SUBMISSION: Need to change to longtable before final submission**

```
A table enviroment cannot be broken across pages. Delete \begin{table}\centering and \end{table}, replace tabular with longtable, move \caption and label to immediately after \begin{longtable}{..}. And add \usepackage{longtable} to the preamble.
```


## Part (c)

**Use the LASSO to determine which covariates (and higher order terms) to include in your regression from part (b). Do you end up dropping some covariates that you had thought might be necessary to include?**

For brevity, I continue on with the exercise with only one dependent variable (dbrwt), skipping the others (fmaps and omaps). Running a simple lasso with all the included varibles from 1(b), and arbitrarily setting number of variables to be included as 20, we chose lambda = 14.23. This is the lowest lambda iteration that results in a model with 20 non-zero coefficients.

Under this setting, predicted impact of smoking on birthweight is -76.9.

We ended up dropping 'dmage' and 'dfage', or age of parents, which we expected to have some correlation with infant's general health and/or weight. On the other hand, variables such as 'csex' and 'preterm', which we expected to have a stronger relationship with infant weight (i.e. male infant might be bigger on average, and previous baby's birth weight might be predictive of the baby of interest), are not dropped as expected.

One problem, however, with what we have done below is that while an interaction term that includes 'dmage' is included, the standalone variable 'dmage' is dropped, which might be undesirable. 


```{r 1C}
# use glmnet with alpha=1 for lasso
reg_1c = glmnet(xx, birthweight, family="gaussian", alpha=1)
# print results (Df = # of variables, %Dev = R^2)
print(reg_1c)
# Limit the model to num_vars number of variables
num_vars = 20
# choose lowest lambda iteration that results in num_vars non-zero variables
i = max(which(abs(reg_1c$df - num_vars) == min(abs(reg_1c$df - num_vars))))
lambda = reg_1c$lambda[[i]]
print(paste0('# of variables in ', i, 'th iteration: ', sum(reg_1c$beta[, i] != 0)))
# print the i'th lasso regression coefficients
# print(reg_1c$beta[, i])

# Plot what the coefficients are doing as we increase lambda
op <- par(mfrow=c(1,2))
plot(reg_1c, "norm", label=TRUE)
plot(reg_1c, "lambda", label=TRUE)
par(op)
```
In both graphs, each curve corresponds to a variable. They show the path of its 
  coefficient against (in the left plot) the L1-norm of the whole coefficient 
  vector as lambda varies and (in the right plot) the values of log-lamba.
  The top axis indicates the number of nonzero coefficients at the current lambda, 
  which is the effective degrees of freedom (df) for the lasso. We see clearly 
  that as lambda increases more coefficients goes to zero, as intended in 
  model selection.

Here are the non-zero coefficients:
```{r 1C table, results='asis'}
knitr::kable(reg_1c$beta[, i][reg_1c$beta[, i] != 0], 
             caption=paste0("Lasso Regression for top ", num_vars, " vars (lambda = ", lambda, ")"),
             col.names = 'Non-zero Coefficients', align = "l", digits = 3)
```

<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 2

**Describe the propensity score approach to the problem of estimating the average causal effect of smoking when the treatment is randomly assigned conditional on the observables. How does it reduce the dimensionality problem of multivariate matching? Try a few ways to estimate the effects of maternal smoking on birthweight:**

## Part (a)

**First create the propensity score. For our purposes let’s use a logit specification. First specify the logit using all of the “predetermined” covariates (don’t include interactions). Next, include only those “predetermined” covariates that enter significantly in the first logit specification. How comparable are the propensity scores? If they are similar does this imply that we have the “correct” set of covariates in the logit specification used for our propensity score?**
```{r 2a1}
# create the propensity score using logit
# using all of the “predetermined” covariates
df2a = df %>% select(tobacco, csex, mrace3, preterm,
                     dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
                     disllb, dtotord, dmar, adequacy, nprevist)
reg_2a1 <- glm(tobacco ~ ., data = df2a, family = "binomial")
```

```{r 2a1 table, results='asis'}
stargazer(reg_2a1, title="First logit regression", header=FALSE, single.row=TRUE, type=table_type)
```

Removing sex of the child (`csex`) and prenatal adequacy (`adequacy`) from regressors (not significant).

```{r 2a2}
# Try logit with only the significant covariates
df2a2 = df2a %>% select(-csex, -adequacy)
reg_2a2 <- glm(tobacco ~ ., data = df2a2, family = "binomial")
```

```{r 2a2 table, results='asis'}
stargazer(reg_2a2, title="Second logit regression", header=FALSE, single.row=TRUE, type=table_type)
```

```{r Showing p-scores with actual data values HH}
p1_df <- data.frame(p1_score = predict(reg_2a1, type = "response"), tobacco = reg_2a1$model$tobacco)
p2_df <- data.frame(p2_score = predict(reg_2a2, type = "response"), tobacco = reg_2a2$model$tobacco)
```

```{r Histograms HH}
labs <- paste("Actual smoking status:", c("Yes", "No"))
p1_df %>%
  mutate(tobacco = ifelse(tobacco == 1, labs[1], labs[2])) %>%
  ggplot(aes(x = p1_score)) +
  geom_histogram(color = "white") +
  facet_wrap(~tobacco) +
  xlab("Probability of maternal smoking") +
  theme_bw()

p2_df %>%
  mutate(tobacco = ifelse(tobacco == 1, labs[1], labs[2])) %>%
  ggplot(aes(x = p2_score)) +
  geom_histogram(color = "white") +
  facet_wrap(~tobacco) +
  xlab("Probability of maternal smoking") +
  theme_bw()
```

```{r 2a pscores}
# Compare histograms of p-scores
p1 = reg_2a1 %>% predict(df2a, type = "response")
p2 = reg_2a2 %>% predict(df2a2, type = "response")
```

The histogram of the first predicted p-score is in orange, the second is in purple.
So this fully-pink histogram is meant to show that there is nearly complete
overlap in this histograms of the predicted p-scores.
```{r}
plot(hist(p1, plot=F), col=rgb(0,0,1,1/4), border=NA, xlim=c(0,1))  # first histogram
plot(hist(p2, plot=F), col=rgb(1,0,0,1/4), border=NA, xlim=c(0,1), add=T)  # second
```

We can see where the differences are that are more than 0.001 and the shape of the 
differences.
```{r}
threshold = 0.001
pdiff = p1-p2
plot(hist(p1[abs(pdiff) > threshold], plot=F), col=rgb(0,0,1,1/4), border=NA, xlim=c(0,1),
     main=paste('Histogram of p-scores that have differences greater than', threshold))
plot(hist(pdiff, plot=F), col=rgb(0,0,1,1/4), border=NA, main='Histogram of differences in p-scores')
```

Here's two density plots to reinforce the idea. The blue line is the density plot
for the first p-score and the red filling is the density plot for the second.
```{r}
plot(density(p1), col=rgb(0,0,1), xlim=c(0,1), lwd=3,
     main='Comparing p-score densities with and without significant variables')  # first kernel
polygon(density(p2), col="red", border=NA, xlim=c(0,1))  # second kernel filled in
```

Note that the maximum difference between any two predicted p-scores is `r max(abs(p1-p2))`.

Overall, the propensity scores are very similar. However, this does not imply that we have the "correct" set of covariates in the logit specification used for our propensity score. It merely shows that excluding the non-significant variables from the propensity score estimation won't affect the propensity score estimates. This doesn't suggest that we aren't missing any potentially important covariates. 

## Part (b)

**Control directly for the estimated propensity scores using a regression analysis, and estimate an average treatment effect. State clearly the assumptions under which your estimate is correct.**


```{r}
# Control for p-score in regression analysis
df2b = df %>%
  select(dbrwt, tobacco, csex, mrace3, preterm,
         dmage, dfage, dmeduc, dfeduc, ormoth, orfath,
         disllb, dtotord, dmar, adequacy, nprevist) %>%
  mutate(pscore = p1)
reg_2b = lm(dbrwt ~ ., data=df2b)
summary(reg_2b)
# Estimate ATE
ATE_2b = reg_2b$coefficients["tobacco1"]

reg_2bb = lm(dbrwt ~ tobacco + pscore, data=df2b)
ATE_2bb = reg_2bb$coefficients["tobacco1"]
```

The estimated ATE of tobacco use during pregnancy when including covariates and the pscore is `r round(ATE_2b,3)` -- smoking during pregnancy causes a `r abs(round(ATE_2b,3))` drop in grams of the birthweight of the child. When only including the pscore, the ATE is `r round(ATE_2bb,3)`. This is correct if we have unconfoundedness and we assume a constant treatment effect. 


## Part (c)

**As discussed in class, one can use the estimated propensity scores to reweight the outcomes of non-smokers and estimate the average treatment effect. Compute an estimate of the average treatment effect and the “effect of the treatment on the treated” by appropriate reweighting of the data.**

Imbens tells us that, after dividing by the sum of the weights in each treatment group, the feasible propensity-score-weighted ATE is:
\[
\hat\tau_{ATE} = \frac{\sum\limits_{i=1}^N\dfrac{Y_i\cdot D_i}
   {\hat p(X_i)}}{\sum\limits_{i=1}^N\dfrac{D_i}{\hat p(X_i)}} - 
   \frac{\sum\limits_{i=1}^N\dfrac{(1-D_i)\cdot Y_i}{1-\hat p(X_i)}}
   {\sum\limits_{i=1}^N\dfrac{1-D_i}{1-\hat p(X_i)}}
\]

```{r}
# Reweight data using p-score to weight
df2c = df2b %>% 
  mutate(pscore = predict(reg_2a1, ., type = "response")) %>%
  mutate(weight = ifelse(tobacco == 1, 1/pscore, 1/(1-pscore)))
t_norm1 = sum((df2c$tobacco==1)*df2c$weight)  # smokers
c_norm1 = sum((df2c$tobacco==2)*df2c$weight)  # non-smokers
df2c = df2c %>% mutate(weight3 = ifelse(tobacco == 1, 1/pscore/t_norm1, 1/(1-pscore)/c_norm1))

# Estimate ATE
weight_mean_smoker_all = sum((df2c$tobacco==1) * df2c$weight * df2c$dbrwt) / t_norm1
weight_mean_nonsmoker_all = sum((df2c$tobacco==2) * df2c$weight * df2c$dbrwt) / c_norm1
ATE2c = weight_mean_smoker_all - weight_mean_nonsmoker_all
ATE2c
```
The propensity-score-weighted estimated average treatment effect is `r round(ATE2c,2)`.

Imbens (2004) in a review article states that the efficient TOT estimator of $\tau_{treated}$ is derived by weighting each observation by it's propensity score. This makes intuitive sense because we want to weight-up the observations that look like the treated observations and weight-down the observations that look like the untreated. After canceling out the propensity score in the first term and dividing by the sum of the weights for each treatment group, we get:
\[
\hat\tau_{ATT} = \frac{1}{N_T}\sum\limits_{i:D_i=1}Y_i - 
   \frac{\sum\limits_{i:D_i=0}Y_i\dfrac{\hat p(X_i)}{1-\hat p(X_i)}}
   {\sum\limits_{i:D_i=0}\dfrac{\hat p(X_i)}{1-\hat p(X_i)}}
= \frac{\sum\limits_{i=1}^ND_iY_i}{\sum\limits_{i=1}^ND_i} - 
   \frac{\sum\limits_{i=1}^N\dfrac{(1-D_i)\hat p(X_i)Y_i}{1-\hat p(X_i)}}
   {\sum\limits_{i=1}^N\dfrac{(1-D_i)\hat p(X_i)}{1-\hat p(X_i)}}
\]
```{r}
# Reweight data for ATT
df2c = df2c %>% 
  mutate(weight2 = ifelse(tobacco == 1, 1, pscore/(1-pscore)))
t_norm2 = sum((df2c$tobacco==1)*df2c$weight2)  # smokers
c_norm2 = sum((df2c$tobacco==2)*df2c$weight2)  # non-smokers

# Estimate ATT
weight_mean_smoker_treat = sum((df2c$tobacco==1) * df2c$weight2 * df2c$dbrwt) / t_norm2
weight_mean_nonsmoker_treat = sum((df2c$tobacco==2) * df2c$weight2 * df2c$dbrwt) / c_norm2
ATT2c = weight_mean_smoker_treat - weight_mean_nonsmoker_treat
ATT2c
```
The propensity-score-weighted estimated average treatment effect on the treated is `r round(ATT2c,2)`. If we take this result as significantly different from the ATE, then the effect of smoking on birthweight is stronger for those who look like smokers than it is in the general population. This seems to suggest that there are other, possibly unobserved, characteristics that decrease birthweight that are more common in the population that look like non-smokers than in population that looks like smokers.

## Part (d)

**Estimate the counterfactual densities relevant for the above part with a kernel density estimator. That is, estimate the density of birthweight (or log birthweight) if everyone smoked and again if no one smoked. Hint: Consider directly applying the Hirano, Imbens, and Ridder propensity score reweighting scheme in the context of estimating the densities of the treated and control groups (rather than the means of the treated and control groups). Stata has very useful preprogrammed commands. In addition to using the preprogrammed Stata command to compute/graph the kernel density over the entire range of birthweight, please also calculate by hand the kernel estimator at birthweight equals 3,000 grams (and provide the code you wrote that shows the calculation of the kernel estimator at this single point). Play around with a bandwidth starting with half the default Stata bandwidth. Choose the same bandwidth for all the pictures, and produce a (beautiful, production quality) figure depicting both densities.**

Morgan and Todd (2008) gives us the weights for ATT and ATC (average treatment effect on the controls). These come from weighting observations by $1-p(X_i)$ instead of $p(X_i)$ to weight toward the control group:

\[
\begin{aligned}
\text{For }D_i=1: &\quad w_{i,ATT} = 1\\
\text{For }D_i=0: &\quad w_{i,ATT} = \frac{\hat p(X_i)}{1-\hat p(X_i)}\\
\text{and} \qquad&\\
\text{For }D_i=1: &\quad w_{i,ATC} = \frac{1-\hat p(X_i)}{\hat p(X_i)}\\
\text{For }D_i=0: &\quad w_{i,ATC} = 1 \\
\end{aligned}
\]

We can use weighted kernel density estimation with the weights above:

```{r Problem 2d}
# Estimate the counterfactual birthweight densities with a kernel density estimator
# See Joel's notes for kernel density estimator
# Play around with a bandwidth starting with half the default Stata bandwidth
# For stata bandwidht, see rkdensity.pdf page 9 in this ps1b github folder.
# You can also run on stata with no bandwidth specified, then print the
# default bandwidth used using `display r(bwidth)`
# Choose the same bandwidth for all the pictures

# Stata default bandwidth
m = min(var(df2c$dbrwt), IQR(df2c$dbrwt)/1.349)
h_stata = 0.9*m/nrow(df2c)^(1/5)  # 44.14637 grams
df2d = df2c

# Create a vector of birthwieght values for plotting the density
x_vec = seq(min(df2d$dbrwt), max(df2d$dbrwt), length.out=1000)

# Define the kernel
kernel_fun <- function(x, h){ # Epanechnikov Kernel with bandwidth h
  weight = (3/4)*(1-(x/h)^2)*as.numeric(abs(x/h)<1)
  return(weight)
}

# Define the kernel density
density_fun <- function(x, df, h) { # Kernel density estimate with data X and weights w
  withCallingHandlers({
    # h = h_stata*2
    n = nrow(df)
    df = df %>% filter(between(dbrwt, x-h, x+h))
    if (nrow(df) == 0) {return(0)}
    kernel_vec = sapply(x - df$dbrwt, kernel_fun, h=h)
    f = sum(df$weight * kernel_vec) / (n*h) #
    return(f)
  }, warning=function(w) {
      if (startsWith(conditionMessage(w), "between() called on numeric"))
          invokeRestart("muffleWarning")
  })
}

# Calculate starting bandwidths (based on Stata formula)
m1 = min(var(filter(df2d, tobacco==1)$dbrwt), IQR(filter(df2d, tobacco==1)$dbrwt)/1.349)
h1 = 0.9*m/nrow(filter(df2d, tobacco==1))^(1/5)
m2 = min(var(filter(df2d, tobacco==2)$dbrwt), IQR(filter(df2d, tobacco==2)$dbrwt)/1.349)
h2 = 0.9*m/nrow(filter(df2d, tobacco==2))^(1/5)
# estimate counterfactual densities (takes about 16 seconds for both over 1000 point x_vec)
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=2*h1)
dens2_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==2), h=2*h2)

# Caclulate area under the curve to use as normalization constants for the PDFs
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
norm_cons2 = sum(diff(x_vec) * zoo::rollmean(dens2_custom, 2))

# Use normalization constants to scale up to true PDFs
dens_df = rbind(data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes"),
                data.frame(birthweight=x_vec, f=(dens2_custom / norm_cons2), smoker="no"))

# Plot
mean_lines = data.frame(avg = c(weight_mean_smoker_all, weight_mean_nonsmoker_all),
    name = c(paste("weighted mean smoker-babies weight =", round(weight_mean_smoker_all, 1)),
             paste("weighted mean nonsmoker-babies weight =", round(weight_mean_nonsmoker_all, 1))))
p <- ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5) +
  geom_vline(data=mean_lines, mapping=aes(xintercept=avg), color="blue") +
  geom_text(data=mean_lines, mapping=aes(x=avg, y=0, label=name), size=4, angle=90, vjust=c(-0.4, 1.2), hjust=0)
p
```

Calculate the kernel estimator at birthweight equals 3,000 grams:
```{r Problem 2d cont.}
# for smokers
density_fun(3000, filter(df2d, tobacco==1), 2*h1) / norm_cons1
# for nonsmokers
density_fun(3000, filter(df2d, tobacco==2), 2*h2) / norm_cons1
```


## Part (e)
**Take one of your densities and display an estimate of the density using different bandwidths as well as the one you settled on. What happens with bigger (smaller) bandwidths?**

Smokers' density with bandwidth = 1/2 * Stata bandwidht
```{r Problem 2e 0.5, echo=False}
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=0.5*h1)
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
dens_df = data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes")
ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5)
```

Smokers' density with bandwidth = 1 Stata bandwidht
```{r Problem 2e 0.5, echo=False}
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=h1)
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
dens_df = data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes")
ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5)
```

Smokers' density with bandwidth = 2 Stata bandwidht
```{r Problem 2e 0.5, echo=False}
dens1_custom = sapply(x_vec, density_fun, df=filter(df2d, tobacco==1), h=2*h1)
norm_cons1 = sum(diff(x_vec) * zoo::rollmean(dens1_custom, 2))
dens_df = data.frame(birthweight=x_vec, f=(dens1_custom / norm_cons1), smoker="yes")
ggplot(dens_df, aes(x = birthweight, y = f)) +
  geom_polygon(aes(fill=smoker, group = smoker), alpha = 0.5)
```

As we increase the bandwidth, the density esimtate gets smoother and smoother.


## Part (f)
**What are the benefits of the weighting approach (from part c)? What are the potential drawbacks? Pay particular attention to to the issue of people with extremely high and extremely low values of the propensity score.**

```{r Problem 2f}

```



## Part (g)

**Present your findings and interpret the results on the relationship between birthweight and smoking. For the estimates in parts (b) and (c), consider which of the following conditions must hold in order for that estimate to be valid:**

- The treatment effect heterogeneity is linear in the propensity score.
- The treatment effect heterogeneity is not linear in the propensity score.
- The decision to smoke is completely randomly assigned.
- Conditional on the exogenous variables the decision to smoke is randomly assigned.


```{r Problem 2g}

```



<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 3
**A potentially more informative way to describe how birth weight affects smoking is to estimate the “non-parametric” conditional mean of birth weight as a function of the estimated probability of smoking, separately for smokers and non-smokers on the same graph. To do so, divide the data from smokers into 100 approximately equally spaced bins based on the estimated propensity score. Do the same for nonsmokers. Use the blocking estimator we discussed in class. Interpret your findings and relate them to the results in (2b).**

```{r Problem 3}
#Prepping dataset arranged by pscore
df3 <- df %>% 
  select(dbrwt, tobacco) %>%
  mutate(pscore = p1, smoke = ifelse(tobacco == 1, 1,0)) %>%
  select(-tobacco) %>% 
  arrange(pscore)

# Check Overlap Assumption
minmax <- df3 %>% 
            group_by(smoke) %>%
            summarise(max = max(pscore), min = min(pscore))

#Creating bins separately for smokers and non-smokers
df3_smoke <- df3 %>%
  filter(smoke == 1) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

df3_nosmoke <- df3 %>%
  filter(smoke == 0) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

#Generating t_k (treatment effects for bins)
df3_smoke_mean <- df3_smoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_smoke = mean(dbrwt), N_smoke = n())

df3_nosmoke_mean <- df3_nosmoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_nosmoke = mean(dbrwt), N_nosmoke = n())

#Generating t (weighted total treatment effect)
df3_merged <- full_join(df3_nosmoke_mean, df3_smoke_mean, by = "pscore_bin") %>% 
  filter(is.na(bin_mean_nosmoke) == 0 & is.na(bin_mean_smoke) == 0) %>%
  mutate(dbrwt_diff = bin_mean_smoke - bin_mean_nosmoke, N = N_smoke + N_nosmoke)

N_total_3 <- sum(df3_merged$N)

df3_merged <- df3_merged %>% 
  mutate(t_k_weighted = dbrwt_diff * (N/N_total_3))  

t_3 <- sum(df3_merged$t_k_weighted)

```

First, for each group of smokers and nonsmokers, we see that the range of estimated propesntiy score is (`r round(as.numeric(minmax[1,3]), 3)`, `r round(as.numeric(minmax[1,2]), 4)`) for nonsmokers and (`r round(as.numeric(minmax[2,3]), 4)`, `r round(as.numeric(minmax[2,2]), 4)`) for smokers. The range is very similar, meaning overlap assumption is satisfied, and trimming for blocking is not needed. This also ensures that what we have done above, which is creating 100 equal bins between the minimum and maximum propensity score for each group, will be very similar to creating the same 100 bins for the two groups such that first bin is [0, 0.01] and so on.

Using the blocking estimator, we find a smoking treatment effect of `r round(t_3,3)`


<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 4
**Low birth weight births (less than 2500 grams) are considered particularly undesirable since they comprise a large share of infant deaths. Redo question 3 using an indicator for low birth weight birth as the outcome of interest. Interpret your findings.**

```{r Problem 4}
# Prepping dataset with indicator variable 
df4 <- df3 %>% 
  mutate(low_dbrwt = ifelse(dbrwt < 2500, 1, 0))

# Creating bins separately for smokers and non-smokers
df4_smoke <- df4 %>%
  filter(smoke == 1) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

df4_nosmoke <- df4 %>%
  filter(smoke == 0) %>%
  mutate(pscore_bin = cut(pscore, breaks = 100, labels = seq(1,100)))

# Generating t_k (treatment effects for bins)
df4_smoke_mean <- df4_smoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_smoke = mean(low_dbrwt), N_smoke = n())

df4_nosmoke_mean <- df4_nosmoke %>% 
  group_by(pscore_bin) %>% 
  summarise(bin_mean_nosmoke = mean(low_dbrwt), N_nosmoke = n())

# Generating t (weighted total treatment effect)
df4_merged <- full_join(df4_nosmoke_mean, df4_smoke_mean, by = "pscore_bin") %>% 
  filter(is.na(bin_mean_nosmoke) == 0 & is.na(bin_mean_smoke) == 0) %>%
  mutate(low_dbrwt_diff = bin_mean_smoke - bin_mean_nosmoke, N = N_smoke + N_nosmoke)

N_total_4 <- sum(df4_merged$N)

df4_merged <- df4_merged %>% 
  mutate(t_k_weighted = low_dbrwt_diff * (N/N_total_4))  

t_4 <- sum(df4_merged$t_k_weighted)
```

Using the blocking estimator, we find a smoking treatment effect of `r round(t_4,3)`. That is smokers are `r round(t_4,3)*100` percentage points more likely to have a baby with low birth weight than non-smokers. 

<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 5
**Let’s link matching back to regression. Consider the conditional expectation function $\mathbb{E}[birthweight\ |\ X]$, where $X$ contains the following variables: \texttt{rectype pldel3 cntocpop stresfip dmage mrace3 dmar adequacy csex dplural}.**

## Part (a)
**Develop a regression that you are confident estimates $\mathbb{E}[birthweight\ |\ X]$ as $N\to\infty$? Why are you confident that your regression gets the CEF right?**

```{r Problem 5a (try 1), eval=F}
# Select variables
df5a = df %>% 
          select(dbrwt, rectype, pldel3, cntocpop, stresfip, 
                 dmage, mrace3, dmar, adequacy, csex, dplural) %>% 
          mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Check to make sure all are dummies
dummies <- c("rectype", "pldel3", "cntocpop", "stresfip", 
             "dmage", "mrace3", "dmar", "adequacy", "csex", "dplural")

for (var in dummies) {
  print(class(df5a[, var]))
} 

  
# Run saturated regression
reg_5a <- lm(dbrwt ~ rectype + pldel3 + cntocpop + stresfip +
               dmage + mrace3 + dmar + adequacy + csex + dplural +
               rectype*pldel3 + rectype*cntocpop + rectype+stresfip + 
               rectype*dmage + rectype*mrace3 + rectype*dmar + 
               rectype*adequacy + rectype*csex + rectype*dplural +
               pldel3*cntocpop + pldel3*stresfip + pldel3*dmage + 
               pldel3*mrace3 + pldel3*dmar + pldel3*adequacy +
               pldel3*csex + pldel3*dplural +
               cntocpop*stresfip + cntocpop*dmage + cntocpop*mrace3 + 
               cntocpop*dmar + cntocpop*adequacy + cntocpop*csex + cntocpop*dplural +
               stresfip*dmage +  stresfip*mrace3 +  stresfip*dmar + 
               stresfip*adequacy +  stresfip*csex +  stresfip*dplural +
               dmage*mrace3 + dmage*dmar + dmage*adequacy + 
               dmage*csex + dmage*dplural + 
               mrace3*dmar + mrace3*adequacy + mrace3*csex + mrace3*dplural +
               dmar*adequacy + dmar*csex + dmar*dplural +
               adequacy*csex + adequacy*dplural +
               csex*dplural,
            data = df5a)

summary(reg_5a)
```

```{r, eval=FALSE}
# The below code is just a helpful reference for using local variable names in for loop
make_dummy <- c("rectype")
for (var in make_dummy) {
  name <- paste0(var, "_1")
  df5a <- df5a %>%
    mutate(!!name := ifelse(df5a[,var] == 1, 1 ,0))
}
```

```{r Problem 5a Contending Version, eval=F}
# Select variables (Error: cannot allocate vector of size 2616.7 Gb)
#df5a = df %>% 
#          select(dbrwt, rectype, pldel3, cntocpop, stresfip, 
#                 dmage, mrace3, dmar, adequacy, csex, dplural) %>% 
#          mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Saturated Regression
#reg_5a_2nd_attempt <- lm(dbrwt ~ (.)^10, data = df5a)
#summary(reg_5a_2nd_attempt)



# Select variables
df5a = df %>% 
          select(dbrwt, tobacco, rectype, pldel3, cntocpop, stresfip, 
                 dmage, mrace3, dmar, adequacy, csex, dplural) %>% 
          mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Create factor variable (for dummies needed for regression) for unique combination of covariate values
df5a_unique = df5a %>% 
                select(-c(dbrwt, tobacco)) %>% 
                distinct() %>% 
                arrange(rectype, pldel3, cntocpop, stresfip, 
                  dmage, mrace3, dmar, adequacy, csex, dplural) %>%
                mutate(uniq = row_number())
df5a <- full_join(df5a, df5a_unique, by = c("rectype", "pldel3", "cntocpop", 
                  "stresfip", "dmage", "mrace3", "dmar", "adequacy", "csex", "dplural"))
df5a <- df5a %>% mutate(uniq = as.factor(uniq))

# Run regression on dbrwt with saturated model
reg_5a <- lm(dbrwt ~ uniq, data = df5a)
summary(reg_5a)
```

Saturated model for discrete regressors is a sufficient condition for a linear CEF. Hence, I would create a dummy variable for each unique combination of covariate values and run linear regression on these newly created dummy variables to estimate CEF.



## Part (b)
**Now run the regression you propose above, but add the treatment (your binary smoking variable) as the righthand side variable of interest. Prove that if the treatment effect of smoking on birthweight is independent of the covariates in $X$, then exact matching and your regression estimate the same thing. You may assume the conditional independence assumption holds given the variables in $X$ listed above.**

```{r Problem 5b (try 1), eval=F}
# Select vars and smoking indicator
df5b = df %>% 
  select(dbrwt, tobacco, rectype, pldel3, cntocpop, stresfip, 
                     dmage, mrace3, dmar, adequacy, csex, dplural) %>%
  mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Check to make sure all are dummies
dummies <- c("tobacco", "rectype", "pldel3", "cntocpop", "stresfip", 
                "dmage", "mrace3", "dmar", "adequacy", "csex", "dplural")
for (var in dummies) {
  print(class(df5b[, var]))
}

# Run regression
reg_5b <- lm(dbrwt ~ tobacco + rectype + pldel3 + cntocpop + stresfip +
               dmage + mrace3 + dmar + adequacy + csex + dplural +
               rectype*pldel3 + rectype*cntocpop + rectype+stresfip + 
               rectype*dmage + rectype*mrace3 + rectype*dmar + 
               rectype*adequacy + rectype*csex + rectype*dplural +
               pldel3*cntocpop + pldel3*stresfip + pldel3*dmage + 
               pldel3*mrace3 + pldel3*dmar + pldel3*adequacy +
               pldel3*csex + pldel3*dplural +
               cntocpop*stresfip + cntocpop*dmage + cntocpop*mrace3 + 
               cntocpop*dmar + cntocpop*adequacy + cntocpop*csex + cntocpop*dplural +
               stresfip*dmage +  stresfip*mrace3 +  stresfip*dmar + 
               stresfip*adequacy +  stresfip*csex +  stresfip*dplural +
               dmage*mrace3 + dmage*dmar + dmage*adequacy + 
               dmage*csex + dmage*dplural + 
               mrace3*dmar + mrace3*adequacy + mrace3*csex + mrace3*dplural +
               dmar*adequacy + dmar*csex + dmar*dplural +
               adequacy*csex + adequacy*dplural +
               csex*dplural
               , data = df5b)
summary(reg_5b)

```

```{r Problem 5b 5b Contending Version, eval=F}
# Select variables (Error: cannot allocate vector of size 2616.7 Gb)
#df5a = df %>% 
#          select(dbrwt, rectype, pldel3, cntocpop, stresfip, 
#                 dmage, mrace3, dmar, adequacy, csex, dplural) %>% 
#          mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Saturated Regression
#reg_5a_2nd_attempt <- lm(dbrwt ~ (.)^10, data = df5a)
#summary(reg_5a_2nd_attempt)



# Select variables
df5a = df %>% 
          select(dbrwt, tobacco, rectype, pldel3, cntocpop, stresfip, 
                 dmage, mrace3, dmar, adequacy, csex, dplural) %>% 
          mutate(stresfip = as.factor(stresfip), dmage = as.factor(dmage))

# Create factor variable (for dummies needed for regression) for unique combination of covariate values
df5a_unique = df5a %>% 
                select(-c(dbrwt, tobacco)) %>% 
                distinct() %>% 
                arrange(rectype, pldel3, cntocpop, stresfip, 
                  dmage, mrace3, dmar, adequacy, csex, dplural) %>%
                mutate(uniq = row_number())
df5a <- full_join(df5a, df5a_unique, by = c("rectype", "pldel3", "cntocpop", 
                  "stresfip", "dmage", "mrace3", "dmar", "adequacy", "csex", "dplural"))
df5a <- df5a %>% mutate(uniq = as.factor(uniq))

# Run regression on dbrwt with saturated model (incl tobacco)
lm(dbrwt ~ .^2, data=df5a)
reg_5a <- lm(dbrwt ~ tobacco + uniq, data = df5a)
summary(reg_5a)
```

Let Y = dbrwt, D = tobacco, and X = covariates listed for quesiton 5a. Assume $Y_i = \alpha + \beta D_i + \delta h(x_i) + v_i$ and thereby $Y_i = \alpha + \beta \tilde{D_i} + \delta h(x_i) + u_i$, where $\tilde{D_i} = D_i - \mathbb{E}[D_i|X_i] $ and $u_i = \epsilon_i + \beta \mathbb{E}[D_i|X_i]$. We showed, in class, that $\mathbb{E}[u_i \tilde{D_i}] = 0$ under un-confounded-ness and homogeneous treatment, and can get a consistent estimate of $\beta$ if the CEF is known. While our CEF is not known, given we have linear CEF with a saturated model, we can simply include X's as additional control.

Let matrix $Z = [D \; E[D|X_i]]$.

\[
\begin{aligned}
\tau_{(OLS \; 5b)} &= \frac{\mathbb{E}[Z_i Y_i]}{Var[Z_i]} &  \\
                   &= \frac{\mathbb{E}[\tilde{D_i}Y_i]}{Var[\tilde{D_i}]} & \text{given consistent estimate} \\
                   &= \frac{\mathbb{E}[\mathbb{E}[\tilde{D}Y_i|D_i, X_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}\mathbb{E}[Y_i|D_i, X_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}(\mathbb{E}[Y_i|D_i=0, X_i] + \mathbb{E}[\tau_i|D_i, X_i]D_i)]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}(\mathbb{E}[Y_i|D_i=0, X_i] + \mathbb{E}[\tau_i|X_i]D_i)]}{Var[\tilde{D_i}]} & \text{CIA} \\
                   &= \frac{\mathbb{E}[\tilde{D}\mathbb{E}[Y_i|D_i=0, X_i] + \tilde{D}D_i\mathbb{E}[\tau_i]]}{Var[\tilde{D_i}]} & \text{Trt independent of X} \\
                   &= \frac{\mathbb{E}[D_i\mathbb{E}[Y_i|D_i=0, X_i] -  \mathbb{E}[D_i|X_i]\mathbb{E}[Y_i|D_i=0, X_i] + \tilde{D}D_i\mathbb{E}[\tau_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[D_i\mathbb{E}[Y_i|D_i=0, X_i]] -  \mathbb{E}[\mathbb{E}[D_i|X_i]\mathbb{E}[Y_i|D_i=0, X_i]] + \mathbb{E}[\tilde{D}D_i\mathbb{E}[\tau_i]]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[D_i]\mathbb{E}[Y_i|D_i=0, X_i] -  \mathbb{E}[D_i]\mathbb{E}[Y_i|D_i=0, X_i] + \mathbb{E}[\tilde{D}D_i]\mathbb{E}[\tau_i]}{Var[\tilde{D_i}]} \\
                   &= \frac{\mathbb{E}[\tilde{D}D_i]\mathbb{E}[\tau_i]}{Var[\tilde{D_i}]} \\
                   &= \frac{[\mathbb{E}[D_i^2] - \mathbb{E}[D_i]^2]\mathbb{E}[\tau_i]}{Var[\tilde{D_i}]} \\
                   &= \frac{Var[D_i]\mathbb{E}[\tau_i]}{Var[D_i]} \\
                   &= \mathbb{E}[\tau_i]
\end{aligned}
\]

Hence, we know our regression from 5b estimates Average Treatment Effect.

Furthermore, we have assumed CIA holds, that conditioning on X's, materal tobacco is as good as randomly assigned. Hence, matching exactly on X's and comparing outcomes within the match across treatment and control groups should also derive Average Treatment Effect. Hence, our regression from 5b would estimate the same thing as exact matching.


## Part (c)
**Develop a weighted version of the exact matching estimator that estimates the same thing as the regression above (regardless of whether the treatment effect is independent of covariates).**




## Part (d)
**Estimate the weighted matching estimator you propose. Compare it to the regression estimate from part (b). Are they similar?**

```{r Problem 5d}

```


## Part (e)
**Is the sample size of your regression the same as the sample size of your matching estimator, or does the regression have more observations? If the regression has more observations, why don’t these extra observations influence the treatment effect estimate?**



## Part (f)
**Compute a standard error for your matching estimator using the formula from Imbens (2015). Specifically, note that your matching estimator should have a form
    \[\frac{1}{N_t}\sum_{d_i=1}w_iy_i - \frac{1}{N_c}\sum_{d_i=0}w_iy_i\]
    where $\sum_{d_i=1}w_i=N_t$ and $\sum_{d_i=0}w_i=N_c$. Then the conditional variance is approximately
    \[\sum_i\left(\frac{d_i}{N_t^2} + \frac{1-d_i}{N_c^2}w_i^2\hat\sigma_{d_i}^2(x_i) \right),\]
    where $\hat\sigma_{d_i}^2(x_i)=\frac{1}{2}(y_i - y_{nn(i)})$, and $y_{nn(i)}$ is the nearest neighbor to observation $i$ with the \textit{same} treatment status. Figure out the implicit weights $w_i$ in your estimator from part (d), and compute the conditional variance. Is it close to your regression coefficient variance?**
    
```{r Problem 5f}
# Compute a standard error for your matching estimator using the formula from Imbens (2015).


# compute the conditional variance of estimator from (d)


```




<!-- 
===============================================================================
=============================================================================== 
-->
# Problem 6
**Concisely and coherently summarize your overall results, providing some intuition. Write it like you would the conclusion of a paper. In this summary, describe whether you think your best estimate of the effects of smoking is credibly identified. State why or why not.**















