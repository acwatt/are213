---
title: "ARE 213 PS 3"
author: "S. Sung, H. Husain, T. Woolley, A. Watt"
email: aaron@acwatt.net
date: "2021-12-8"
output:
  pdf_document:
    toc: true
    toc_depth: 2
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{amsmath}
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'ARE_213_PS2b.pdf')) })
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = F)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps2b for ARE 213

Notes: Need to open ps2b.Rmd from the ps2b folder (without Rstudio being started first)
if Rstudio is already started, the working directory will not be set
to the ps2b/ folder

\usepackage{dcolumn}: dcolumn is needed in latex compilation for multicolumn tables
-->

```{r Settings}
# stargazer table type (html, latex, or text)
# Change to latex when outputting to PDF, html when outputting to html
table_type = "latex"

```




```{r packages, include=F}
# install.packages("Synth")
library(tidyverse)
library(haven)
library(stargazer)
library(ggplot2)
library(tinytex)
# library(Synth)
# library(plm)
# library(lmtest)
# library(sandwich)
# library(gridExtra)
# library(grid)
# library(gtable)
# library(fastDummies)
# library(EnvStats)
```



\newpage
<!--=========================================================================-->
# Problem 1
<!--=========================================================================-->
**This question asks you to run OLS regressions that look at whether there is an association between 2000 housing values and whether a census tract contained a hazardous waste site that was placed on the NPL by 2000.**

## Part (a)
**Use the file allsites.dta. This file contains only own tract housing variables (i.e. no 2 mile averages). Use "robust" standard errors for all regressions. First regress 2000 housing prices on whether the census tract had an NPL site in 2000. Include 1980 housing values as a control. Next add housing characteristics as controls. Run a third regression adding economic and demographic variables as controls. Finally run a 4th regression that also includes state fixed effects. Briefly interpret the regressions. Under what conditions will the coeffcients on NPL 2000 status be unbiased?**





\newpage
## Part (b)
**Here we will compare covariates between potential treatment and comparison groups. First, use allcovariates.dta to compare co-variates (i.e. those used in the above regressions) between census tracts with and without a hazardous waste site listed on the NPL by 2000. Next, use sitecovariates.dta to compare covariates between those census tracts with a hazardous waste site that had an HRS test in 1982. Specifically, compare those with sites that scored above 28.5 to those that scored below 28.5. Finally, compare those census tracts with sites between 16.5 and 28.5 to census tracts with sites between 28.5 and 40.5. What conclusions do you draw from these 3 comparisons?**






\newpage
<!--=========================================================================-->
# Problem 2
<!--=========================================================================-->
**This question examines the possibility of using a Regression Discontinuity research design. Note that the rest of the empirical question will use the file 2miledata.dta. The housing variables in this file are 2 mile averages.**


## Part (a)
**Consider the HRS score as the running variable for an RD research design. What assumptions are needed on the HRS score? How do each of the following "facts" impact the appropriateness of these assumptions:**

The assumptions we need to hold for a cutoff to be useful in an RD setting are that 
(1) The probability of treatment (NPL) should change discontinuously at the threshold of running variable (HRS). 
(2) We should not see bunching around the threshold, which may be indicative of manipulation in treatment status.

Assumption (1) is not an huge issue in this setting since the cutoff was strict in one direction. The fact that is is not strict in the other direction, however, makes us think it could be a slight concern. The following could either support assumption (2) or call it into question.

### ------ a.i
**The EPA assertion that \the 28.5 cutoff was selected because it produced a manageable number of sites." **

This might cause one to question assumption (2) because it introduces a sense that 28.5 was chosen intentionally rather than at random. Imagine, for instance, that if the regulators are selecting the cutoff based on manageability, then the regulators might line up all of the projects by their scores and determine the cutoff at a cleanup-cost discontinuity.

### ------ a.ii
**None of the individuals involved in identifying the site, testing the level of pollution, or running the 1982 HRS test knew the cutoff threshold score. **

If true, then the data were not gathered with any knowledge of the cutoff ex ante, which make assumption (2) a little more likely to hold.

### ------ a.iii
**EPA documentation emphasizes that the HRS test is an imperfect scoring measure. **

If the HRS test is an imperfect measure, then this also supports assumption (2). The more imperfect HRS is, the more comparable are the treated and untreated groups and therefore the more "randomly" the threshold is likely to have been assigned.


\newpage
## Part (b)
**Create a histogram of the distribution (i.e. density) of the 1982 HRS scores by dividing the HRS score into non-overlapping bins. Include a vertical line at 28.5. Next, run local linear regressions on either side of 28.5 using the midpoints of the bins as the data. What do you conclude?**





\newpage
<!--=========================================================================-->
# Problem 3
<!--=========================================================================-->
**This question examines the 1st stage equation of an RD design using the 1982 HRS score.**


## Part (a)
**Use a 2SLS (IV) econometric setup that uses whether or not a census tract has a site scoring above/below 28.5 as the instrument. Write down the 1st stage equation. Run the 1st stage regression experimenting with the same set of covariates used in question (1). In addition, run a second specification in which you limit the sample to only those census tracts with sites between 16.5 and 40.5 and run the specification using all of the control variables (we will use this as the size of the bandwidth for the "regression discontinuity" regression). Interpret the results.**



\begin{align*}
\intertext{Equaions for 2SLS Set Up}
\intertext{1st Stage}
NPL2000_{i,s} = \alpha_1 + \beta_{1,1} 1(HRS1982 > 28.5)_i + \beta_{1,2} Covariates_{i} + \beta_{1,3} \theta_s + v_{i,s}
\intertext{2nd Stage}
HousingPrice2000_{i,s} = \alpha_2 + \beta_{2,1} \hat{NPL_{i,2000}} + \beta_{2,2} Covariates_{i} + \beta_{2,3} \theta_s + \epsilon_{i,s}
\intertext{Note: $\theta_s$ is state fixed effects.}
\end{align*}

```{r}
names(twomiledata) <- sub("\\_nbr$", "", names(twomiledata))

# Run 1st stage regression (as written above)
twomiledata$ind_hrsabove = twomiledata$hrs_82 >= 28.5
Formula4 <- formula(paste("npl2000 ~ ind_hrsabove + ", paste(c(HousingChar, EconNDemo), collapse=" + "), " | statefips"))

reg3a_1 <- felm(Formula4, data = twomiledata)

# Run 1st stage regression (limiting to tracts with HRS_82 between 16.5 and 40.5)

twomiledata_subset <- twomiledata[twomiledata$hrs_82 >= 16.5,]
twomiledata_subset <- twomiledata_subset[twomiledata_subset$hrs_82 <= 40.5,]

reg3a_2 <- felm(Formula4, data = twomiledata_subset)

# Output Table
stargazer(reg3a_1,reg3a_2,
          se = list(reg3a_1$rse, reg3a_2$rse),
          keep = "ind_hrsabove", type = "text", omit.stat = "f")

```

From this, we can conclude that NPL2000 is extremely predictive in the first stage! This implies that we were wise to use the IV approach before running the second stage reduced form.

## Part (b)
**Create a graph plotting the the 1982 HRS score against whether a site is listed on the NPL by year 2000 (NPL on the y-axis, HRS on the x -axis). Briefly explain and interpret this graph.**

```{r}
plot(twomiledata$hrs_82, twomiledata$npl2000,
  xlab = "HRS Score in 1982", ylab = "Whether on NPL by Year 2000")
  abline(v = 28.5, lty = 3, col = 2, lwd = 2)
```

Above graph demonstrates that we have a clear break at threshold but the cut-off is not as perfect for strict RD design. Rather, we may want to consider Fuzzy RD design. The few census tracts that has lower than 28.5 points on HRS score in 1982 measure but still manages to get on NPL by 2000 could perhaps (1) receive higher score on on HRS in proceeding years, getting onto NPL before 2000, or (2) could potentially have other reasons, i.e. political connections or pressure, for getting onto the list.

## Part (c)
**Create a graph that plots the 1982 HRS score against 1980 property values (property values on the y-axis, HRS on the x -axis). What do you conclude from this graph?**

```{r}
plot(twomiledata$hrs_82, twomiledata$lnmeanhs8,
  xlab = "HRS Score in 1982", ylab = "Median House Value in 1980")
  abline(v = 28.5, lty = 3, col = 2, lwd = 2)
  abline(lm(lnmeanhs8 ~ hrs_82, data = twomiledata), col = "blue")
   
```

While median house price is definitely correlated with HRS score, it seems to be continuous around the threshold and therefore satisfies our assumptions for RD design.



\newpage
<!--=========================================================================-->
# Problem 4
<!--=========================================================================-->
**Write down the 2nd stage equation (with housing values as the out-come) and the 2 standard assumptions for valid IV estimation. Run 2SLS to get the estimated coefficient on 2000 NPL status. Run the same two specifications as in the previous question. Briefly interpret the results.**

\begin{align*}
\intertext{Equaions for 2SLS Set Up}
\intertext{1st Stage}
NPL2000_{i,s} = \alpha_1 + \beta_{1,1} 1(HRS1982 > 28.5)_i + \beta_{1,2} Covariates_{i} + \beta_{1,3} \theta_s + v_{i,s}
\intertext{2nd Stage}
HousingPrice2000_{i,s} = \alpha_2 + \beta_{2,1} \hat{NPL_{i,2000}} + \beta_{2,2} Covariates_{i} + \beta_{2,3} \theta_s + \epsilon_{i,s}
\intertext{Note: $\theta_s$ is state fixed effects.}
\end{align*}

The standard assumptions for valid IV estimation is (1) Cov(HSR1982, \epsilon_{i,s}) = 0 and (2) Cov(HSR1982, NPL2000) =/= 0.

```{r}
# Run 2SLS (tracts with HRS_82 below 28.5 and above)
twomiledata$PredNPL <- reg3a_1$fitted.values
Formula5 <- formula(paste("lnmdvalhs0 ~ PredNPL + ", paste(c(HousingChar, EconNDemo), collapse=" + "), " | statefips"))
reg4_1 <- felm(Formula5, data = twomiledata)


# Run 2SLS (limiting to tracts with HRS_82 between 16.5 and 40.5)
twomiledata_subset$PredNPL <- reg3a_2$fitted.values
reg4_2 <- felm(Formula5, data = twomiledata_subset)

# Output Table
stargazer(reg4_1, reg4_2,
          se = list(reg4_1$rse, reg4_2$rse),
          keep = "PredNPL", type = "text", omit.stat = "f")
```

In the IV (2SLS) methods, both specification lead to statistically insignificant coefficient. Hence, we observe that impact of being put on NPL list by 2000 leads to little change in value of homes from 1980 to 2000. 


\newpage
<!--=========================================================================-->
# Problem 5
<!--=========================================================================-->
**Write a 1 paragraph conclusion summarizing your findings and interpreting the results. Be sure to comment on how the evidence from this problem set supports the primary research question.**

In the above 2SLS procedure, we find that being placed on NPL by year 2000 seems to have statistically insignificant impact on housing prices. This would give interpretation that WTP or willingness to pay for hazardous waste is very small. However, we should note there could be additional factors why we get insignificant results. For example, if the clean-up has not started in many sites by year 2,000 and clean-up generally takes a long time, then not all the 'benefits' or 'willingness to pay for clean-up' may have been internalized by year 2000. In this case, running the analysis 10 years later with updated housing prices may give different result. Alternatively, clean-up process may cause negative externalities  (i.e. noise or trucks passing through) and depress housing demand while in progress and perhaps after clean-up is finished, given improved amenities, supply of housing increases. In any of these cases, results could be measuring something slightly other than Willingness to Pay for CleanUp.





\newpage
# Appendix A: R Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```





















